{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shashi/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk \n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    " \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Experiment (takes 15% of Total Data if ablation is true)\n",
    "ablation = False\n",
    "ablation_ratio = 0.6\n",
    "\n",
    "# how many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = 120000\n",
    "# Percentile of Padding to use with Tokenized words\n",
    "pad_percentile = 50\n",
    "\n",
    "# Use Keras Tokenizer\n",
    "use_tokenizer = False\n",
    "# Use TF IDF Vectorizer\n",
    "use_tf_idf = True\n",
    "\n",
    "# Text Column name\n",
    "text_col = 'lower_text'\n",
    "\n",
    "# How many PCA Components to consider for modelling\n",
    "pca_components = 700\n",
    "\n",
    "# Seed\n",
    "numpy_seed = 478\n",
    "seed = 7\n",
    "# Number of Splits\n",
    "n_splits = 10\n",
    "# Scoring Criteria\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "# if Debug is True, loads Subsequent Dataframes from Disk.\n",
    "debug = False\n",
    "\n",
    "train_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test DF\n",
    "\n",
    "Create Training and Testing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Automate it to take path based on OS\n",
    "#ROOT_DIR = r'/Users/shabhushan/Desktop/python/python-code/dataset/notracking/participants' # Mac Directory Path\n",
    "ROOT_DIR = r'/home/shashi/Desktop/projects/python-code/dataset/notracking/participants' # Linux Directory Path\n",
    "\n",
    "TRAIN_LABELS = os.path.join(ROOT_DIR, r'train', r'labels', r'labels.csv')\n",
    "TRAIN_TEXT = os.path.join(ROOT_DIR, r'train', r'extracted_data', r'extract_combined.csv')\n",
    "TEST_TEXT = os.path.join(ROOT_DIR, r'test', r'extracted_data', r'extract_combined.csv')\n",
    "\n",
    "SUB = os.path.join(ROOT_DIR, r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in training and testing data\n",
    "# one dataframe for labels another for text features\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target variable to number\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n",
    "\n",
    "Some Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_from_word_list(lst):\n",
    "    temp_set_list = [set(nltk.word_tokenize(words)) for words in lst]\n",
    "\n",
    "    return reduce(lambda x, y: {*x, *y}, temp_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(df):\n",
    "    tokenized_words = [nltk.word_tokenize(words) for words in df]\n",
    "    words_list = reduce(lambda x, y: [*x, *y], tokenized_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorizer.fit_transform(words_list)\n",
    "\n",
    "    return pd.DataFrame(vectorizer.vocabulary_.items(), columns=['Text', 'Frequency']).sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(train_df, test_df):\n",
    "    \"\"\"\n",
    "        Get the TF IDF Vector representation for Train and Test data frame\n",
    "        \n",
    "        Creates the TF-IDF Vector, Fit on Training data and transform both training and test data frames\n",
    "        Also, returns feature names for creating a Dataframe later\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "    X_train = vectorizer.fit_transform(train_df)\n",
    "    \n",
    "    X_test = vectorizer.transform(test_df)\n",
    "\n",
    "    return X_train, X_test, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    \"\"\"\n",
    "        Break a Sentence into words, remove Stop words keeping only alphabet and numbers, remove\n",
    "        punctuations, comma etc. and at the end Lemmatize the words.\n",
    "    \"\"\"\n",
    "    tokenized_words = nltk.word_tokenize(words)\n",
    "    \n",
    "    # Remove Stop words\n",
    "    words = [word for word in tokenized_words if word.lower() not in stop_words and word.lower() in corpus]\n",
    "    \n",
    "    # Remove Digits, Keep only Alpha Numeric words\n",
    "    words = [word for word in words if word.isalnum() and not word.isdigit()]\n",
    "\n",
    "    # Lemmatize based on root word\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    return ' '.join([lemma.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "        Add few extra features to the Data Frame\n",
    "        \n",
    "        text: convert to string\n",
    "        lower_text: lowers the text\n",
    "        total_length: length of the document\n",
    "        capitals: number of capitals in document\n",
    "        caps_vs_length: ratio of capital words to total length\n",
    "        num_words: number of words in document.\n",
    "        num_unique_words: number of unique words in document\n",
    "        words_vs_unique: number of unique words in document\n",
    "        document_type: whether the docoment is pdf, doc or docx\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x:str(x))\n",
    "    df[\"lower_text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    df['total_length'] = df['lower_text'].apply(len)\n",
    "    df['capitals'] = df['lower_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['lower_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
    "    df['document_type'] = df['document_name'].apply(lambda val: val.split(\".\")[-1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_top_words_list(df):\n",
    "    word_list = []\n",
    "\n",
    "    df['lower_text'].map(lambda row: word_list.extend(row.split()))\n",
    "\n",
    "    counter_df = pd.DataFrame.from_dict(Counter(word_list), orient='index').reset_index()\n",
    "\n",
    "    counter_df.columns = ['word', 'frequency']\n",
    "\n",
    "    return counter_df.sort_values(by = 'frequency', ascending = False)\n",
    "\n",
    "def get_top_words(df):\n",
    "    # Segregated Positive and Negative classes\n",
    "    top_counter_df_no = get_top_words_list(df[df.is_fitara == 0])\n",
    "    top_counter_df_yes = get_top_words_list(df[df.is_fitara == 1])\n",
    "    \n",
    "    # Fetch Words in Negative class, which are not in Positive class\n",
    "    exclusive_no = set(top_counter_df_no['word'].values) - set(top_counter_df_yes['word'].values)\n",
    "    # Fetch Words in Positive class, which are not in Negative class\n",
    "    exclusive_yes = set(top_counter_df_yes['word'].values) - set(top_counter_df_no['word'].values)\n",
    "    \n",
    "    # English Words Corpus\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    # Keep valid english words only, remove random words\n",
    "    exclusive_no = [word for word in exclusive_no if word in english_words]\n",
    "    exclusive_yes = [word for word in exclusive_yes if word in english_words]\n",
    "    \n",
    "    # Get the Frequency of corresponding words from Original Dataframe\n",
    "    exclusive_no_df = top_counter_df_no[top_counter_df_no['word'].isin(exclusive_no)]\n",
    "    exclusive_yes_df = top_counter_df_yes[top_counter_df_yes['word'].isin(exclusive_yes)]\n",
    "    \n",
    "    return exclusive_no_df, exclusive_yes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentile_features(df):\n",
    "    df['percentile_99'] = df[text_col].apply(get_positive_words_99)\n",
    "    df['percentile_95'] = df[text_col].apply(get_positive_words_95)\n",
    "    df['percentile_90'] = df[text_col].apply(get_positive_words_90)\n",
    "    df['percentile_85'] = df[text_col].apply(get_positive_words_85)\n",
    "    df['percentile_80'] = df[text_col].apply(get_positive_words_80)\n",
    "    df['percentile_75'] = df[text_col].apply(get_positive_words_75)\n",
    "    df['percentile_70'] = df[text_col].apply(get_positive_words_70)\n",
    "    df['percentile_65'] = df[text_col].apply(get_positive_words_65)\n",
    "    df['percentile_60'] = df[text_col].apply(get_positive_words_60)\n",
    "    df['percentile_55'] = df[text_col].apply(get_positive_words_55)\n",
    "    df['percentile_50'] = df[text_col].apply(get_positive_words_50)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Basic Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tokenization and Lemmatization\n",
    "\n",
    "First, we need to remove the stop words, punctuation characters and all other special characters from the text.\n",
    "Then, we need to lemmatize the word to it's root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# English Stop Words list\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "corpus = set(nltk.corpus.words.words())\n",
    "\n",
    "# Create a Tag Dictionary, Default tag is Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "# Get a Lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 110 ms, total: 4min 1s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Caution: will take time to lemmatize whole Data\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since, we have fetched document type from name and lower case text from text, we could safely remove these two columns\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "test_df.to_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_df = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Primilinary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Positive and Negative classes are size 71% and 29% respectively. Hence, no severe class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.713089\n",
       "1    0.286911\n",
       "Name: is_fitara, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# confirm class distribution\n",
    "# is_fitara - yes: ~29%; no: ~71%\n",
    "train_df['is_fitara'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 25 words \n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_no_df.head(25))\n",
    "#plt.plot()\n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_yes_df.head(25))\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation\n",
    "If ablation is true, use only certain percentage of data for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation:\n",
    "    train_df_no = train_df[train_df.is_fitara == 0]\n",
    "    train_df_yes = train_df[train_df.is_fitara == 1]\n",
    "\n",
    "    # Get 15% of total Records for Ablation\n",
    "    train_df_no_ablation = train_df_no.loc[0:int(len(train_df_no) * ablation_ratio)]\n",
    "    train_df_yes_ablation = train_df_yes.loc[0:int(len(train_df_yes) * ablation_ratio)]\n",
    "    \n",
    "    # Shuffle rows and reset index\n",
    "    train_df = pd.concat([train_df_yes_ablation, train_df_no_ablation]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Split into Test and training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y = train_df['is_fitara']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Percentile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_no_df, exclusive_yes_df = get_top_words(pd.concat([X_train, y_train], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_yes_df_99 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.99)]\n",
    "exclusive_yes_df_95 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.95)]\n",
    "exclusive_yes_df_90 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.90)]\n",
    "exclusive_yes_df_85 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.85)]\n",
    "exclusive_yes_df_80 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.80)]\n",
    "exclusive_yes_df_75 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.75)]\n",
    "exclusive_yes_df_70 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.70)]\n",
    "exclusive_yes_df_65 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.65)]\n",
    "exclusive_yes_df_60 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.60)]\n",
    "exclusive_yes_df_55 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.55)]\n",
    "exclusive_yes_df_50 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.50)]\n",
    "\n",
    "def init_word_dict(df):\n",
    "    df2 = get_top_words_list(df)\n",
    "    \n",
    "    word_dict = dict(zip([word for word in df2.word], \n",
    "                         [re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE) for word in df2.word]))\n",
    "\n",
    "def findWholeWord(word):\n",
    "    return re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_positive_words(text, df):\n",
    "    return int(any([findWholeWord(word)(text) != None for word in df['word']]))\n",
    "\n",
    "def get_positive_words_99(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_99)\n",
    "\n",
    "def get_positive_words_95(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_95)\n",
    "\n",
    "def get_positive_words_90(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_90)\n",
    "\n",
    "def get_positive_words_85(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_85)\n",
    "\n",
    "def get_positive_words_80(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_80)\n",
    "\n",
    "def get_positive_words_75(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_75)\n",
    "\n",
    "def get_positive_words_70(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_70)\n",
    "\n",
    "def get_positive_words_65(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_65)\n",
    "\n",
    "def get_positive_words_60(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_60)\n",
    "\n",
    "def get_positive_words_55(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_55)\n",
    "\n",
    "def get_positive_words_50(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init_word_dict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 48s, sys: 872 ms, total: 50min 49s\n",
      "Wall time: 50min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#X_train = add_percentile_features(X_train)\n",
    "#X_test = add_percentile_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_percentile.csv'))\n",
    "#y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_percentile.csv'))\n",
    "\n",
    "#X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_percentile.csv'))\n",
    "#y_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_label_percentile.csv'))\n",
    "\n",
    "X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_percentile.csv'), index_col='Unnamed: 0')\n",
    "y_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_label_percentile.csv'), header=None)\n",
    "\n",
    "X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_percentile.csv'), index_col='Unnamed: 0')\n",
    "y_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_label_percentile.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF\n",
    "Create TF IDF Vector representation for Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tf_idf:\n",
    "    X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "    X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "    X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "    X_train.drop([text_col], axis=1, inplace=True)\n",
    "    X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "if use_tokenizer:\n",
    "    cols = X_train.columns\n",
    "elif use_tf_idf:\n",
    "    cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'))\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'), index_col='Unnamed: 0')\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'), index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aa', 'aam', 'abandon', 'abandonment', 'abase', 'abate', 'abatement',\n",
      "       'abattoir', 'abb', 'abbreviation',\n",
      "       ...\n",
      "       'zero', 'zimbabwe', 'zinc', 'zip', 'zone', 'zoning', 'zoo', 'zoom',\n",
      "       'zoonotic', 'zoster'],\n",
      "      dtype='object', length=10447)\n",
      "Variance Explained by Model is 0.9919467541733579\n"
     ]
    }
   ],
   "source": [
    "standard_columns = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique', \n",
    "        'document_type_doc', 'document_type_docx', 'document_type_pdf' , 'percentile_99', 'percentile_95', 'percentile_90','percentile_85', 'percentile_80', 'percentile_75', 'percentile_70','percentile_65', 'percentile_60', 'percentile_55', 'percentile_50']\n",
    "\n",
    "cols = X_train.columns[len(standard_columns):]\n",
    "\n",
    "print(cols)\n",
    "pca = PCA(n_components = 500)\n",
    "\n",
    "pca.fit(X_train[cols])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[cols]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[cols]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Columns\n",
    "X_train.drop(cols, axis=1, inplace=True)\n",
    "X_test.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'percentile_train_pca.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'percentile_train_label_pca.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'percentile_test_pca.csv'))\n",
    "y_test.to_csv(os.path.join(ROOT_DIR, r'test', r'percentile_test_label_pca.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_label_pca.csv'), header=None, index_col='Unnamed: 0')\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_label_pca.csv'), header=None, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>document_type_doc</th>\n",
       "      <th>document_type_docx</th>\n",
       "      <th>document_type_pdf</th>\n",
       "      <th>percentile_99</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoonotic</th>\n",
       "      <th>zoster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>-0.520329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.518439</td>\n",
       "      <td>-0.625689</td>\n",
       "      <td>0.637329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.095669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.018445</td>\n",
       "      <td>3.034662</td>\n",
       "      <td>-1.191917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>0.770093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>1.133692</td>\n",
       "      <td>-1.310530</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.265650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257545</td>\n",
       "      <td>0.707135</td>\n",
       "      <td>-0.964383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>-0.436256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.433731</td>\n",
       "      <td>-0.421567</td>\n",
       "      <td>-0.225747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.539827</td>\n",
       "      <td>-0.708767</td>\n",
       "      <td>1.031279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>-0.487439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.486910</td>\n",
       "      <td>-0.533231</td>\n",
       "      <td>0.222068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>-0.423784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.397578</td>\n",
       "      <td>-0.419333</td>\n",
       "      <td>-0.666668</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.629416</td>\n",
       "      <td>-0.750490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>-0.093964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089959</td>\n",
       "      <td>0.374375</td>\n",
       "      <td>-0.535549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 10467 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_length  capitals  caps_vs_length  num_words  num_unique_words  \\\n",
       "837     -0.520329       0.0             0.0  -0.518439         -0.625689   \n",
       "722      2.095669       0.0             0.0   2.018445          3.034662   \n",
       "754      0.770093       0.0             0.0   0.913613          1.133692   \n",
       "357      0.265650       0.0             0.0   0.257545          0.707135   \n",
       "506     -0.436256       0.0             0.0  -0.433731         -0.421567   \n",
       "..            ...       ...             ...        ...               ...   \n",
       "372     -0.543057       0.0             0.0  -0.539827         -0.708767   \n",
       "249     -0.487439       0.0             0.0  -0.486910         -0.533231   \n",
       "585     -0.423784       0.0             0.0  -0.397578         -0.419333   \n",
       "889      0.081914       0.0             0.0   0.098901          0.629416   \n",
       "736     -0.093964       0.0             0.0  -0.089959          0.374375   \n",
       "\n",
       "     words_vs_unique  document_type_doc  document_type_docx  \\\n",
       "837         0.637329                  0                   0   \n",
       "722        -1.191917                  0                   0   \n",
       "754        -1.310530                  0                   0   \n",
       "357        -0.964383                  1                   0   \n",
       "506        -0.225747                  0                   0   \n",
       "..               ...                ...                 ...   \n",
       "372         1.031279                  0                   0   \n",
       "249         0.222068                  0                   0   \n",
       "585        -0.666668                  0                   1   \n",
       "889        -0.750490                  0                   1   \n",
       "736        -0.535549                  0                   0   \n",
       "\n",
       "     document_type_pdf  percentile_99  ...  zero  zimbabwe      zinc  \\\n",
       "837                  1              1  ...   0.0       0.0  0.000000   \n",
       "722                  1              0  ...   0.0       0.0  0.000000   \n",
       "754                  1              0  ...   0.0       0.0  0.000000   \n",
       "357                  0              0  ...   0.0       0.0  0.000000   \n",
       "506                  1              0  ...   0.0       0.0  0.000000   \n",
       "..                 ...            ...  ...   ...       ...       ...   \n",
       "372                  1              0  ...   0.0       0.0  0.000000   \n",
       "249                  1              0  ...   0.0       0.0  0.000000   \n",
       "585                  0              0  ...   0.0       0.0  0.000000   \n",
       "889                  0              1  ...   0.0       0.0  0.013084   \n",
       "736                  1              0  ...   0.0       0.0  0.000000   \n",
       "\n",
       "          zip      zone  zoning  zoo  zoom  zoonotic  zoster  \n",
       "837  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "722  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "754  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "357  0.004522  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "506  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "..        ...       ...     ...  ...   ...       ...     ...  \n",
       "372  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "249  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "585  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "889  0.000000  0.020108     0.0  0.0   0.0       0.0     0.0  \n",
       "736  0.000000  0.000000     0.0  0.0   0.0       0.0     0.0  \n",
       "\n",
       "[764 rows x 10467 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "First Try fitting a Simple Neural Network for benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check cross validation score for different algorithms on training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB :: -0.092080 ( +- 0.038504) \n",
      " Log Loss : 1.583790\n",
      " Accuracy : 64.920000\n",
      "\n",
      "GBC :: -0.128213 ( +- 0.040266) \n",
      " Log Loss : 1.480969\n",
      " Accuracy : 64.400000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "estimators = []\n",
    "\n",
    "#estimators.append(('LR', LogisticRegression()))\n",
    "#estimators.append(('NB', BernoulliNB(alpha=.01)))\n",
    "#estimators.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#estimators.append(('KNN', KNeighborsClassifier()))\n",
    "#estimators.append(('TREE', DecisionTreeClassifier()))\n",
    "#estimators.append(('CART', RandomForestClassifier(n_estimators=100)))\n",
    "#estimators.append(('NB', GaussianNB()))\n",
    "#estimators.append(('SVM', SVC(probability=True)))\n",
    "estimators.append(('XGB', XGBClassifier()))\n",
    "estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    #cv_results = model_selection.cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    log_loss = metrics.log_loss(y_test, model.predict_proba(X_test))\n",
    "    accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n Log Loss : %f\\n Accuracy : %f\\n\" % (name, cv_results.mean(), cv_results.std(), log_loss, round(accuracy*100, 2))\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction No</th>\n",
       "      <th>Prediction Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>89</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction No  Prediction Yes\n",
       "Actual No              89              58\n",
       "Actual Yes             10              34"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, estimators[1][1].predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n",
    "\n",
    "#print(metrics.accuracy_score(y_test.values, estimators[2][1].predict(X_test)))\n",
    "#print(metrics.log_loss(y_test.values, estimators[4][1].predict_proba(X_test)))\n",
    "#math.exp(-metrics.log_loss(y_test.values, estimators[5][1].predict_proba(X_test)))\n",
    "\n",
    "#y_pred = estimators[2][1].predict(X_test)\n",
    "#metrics.roc_curve(y_test.values, y_pred)\n",
    "#estimators[2][1].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: -0.130605 (+/- 0.044564)\n",
      "1.558464576064756\n",
      "0.643979057591623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction No</th>\n",
       "      <th>Prediction Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>89</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction No  Prediction Yes\n",
       "Actual No              89              58\n",
       "Actual Yes             10              34"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(ensemble, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(metrics.log_loss(y_test, ensemble.predict_proba(X_test)))\n",
    "print(metrics.accuracy_score(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, ensemble.predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000, # Very large number\n",
    "    seed=7,\n",
    "    reg_alpha=5,\n",
    "    eval_metric='auc',\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_set=[(X_train, y_train)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(metrics.log_loss(y_test, clf.predict_proba(X_test)))\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, ensemble.predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df\n",
    "\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)\n",
    "\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "X_train = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y_train = train_df['is_fitara']\n",
    "\n",
    "X_test = test_df.drop(['document_name'], axis=1)\n",
    "\n",
    "# Percentile Features\n",
    "exclusive_no_df, exclusive_yes_df = get_top_words(pd.concat([X_train, y_train], axis=1))\n",
    "init_word_dict(X_train)\n",
    "X_train = add_percentile_features(X_train)\n",
    "X_test = add_percentile_features(X_test)\n",
    "\n",
    "# TF - IDF Vectorization\n",
    "X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "X_train.drop([text_col], axis=1, inplace=True)\n",
    "X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_X.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_X.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_X.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Features\n",
    "cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PCA\n",
    "columns = X_train.columns[len(cols):]\n",
    "\n",
    "pca = PCA(n_components = pca_components)\n",
    "\n",
    "pca.fit(X_train[columns])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[columns]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[columns]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")\n",
    "\n",
    "# Remove Old Columns\n",
    "X_train.drop(columns, axis=1, inplace=True)\n",
    "X_test.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "\n",
    "\n",
    "test_estimators = []\n",
    "\n",
    "#test_estimators.append(('LR', LogisticRegression()))\n",
    "test_estimators.append(('XGB', XGBClassifier()))\n",
    "test_estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in test_estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(test_estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(test_estimators, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', verbose=2, n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ensemble.predict_proba(X_test))\n",
    "#sub = pd.read_csv(SUB)\n",
    "\n",
    "# Add prediction on Test DF\n",
    "test_df['pred_fitara'] = round(df.iloc[:, 1], 2)\n",
    "\n",
    "submission = test_df.loc[:, ['document_name', 'pred_fitara']]\n",
    "\"\"\"\n",
    "submission = pd.merge(\n",
    "    sub,\n",
    "    test_df,  \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ").loc[:, ['document_name', 'pred']]\n",
    "\n",
    "submission.columns = ['document_name', 'pred_fitara']\n",
    "\"\"\"\n",
    "\n",
    "submission.to_csv(os.path.join(ROOT_DIR, r'submission-4-ensemble-percentile-.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
