{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk \n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    " \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Experiment (takes 15% of Total Data if ablation is true)\n",
    "ablation = False\n",
    "ablation_ratio = 0.6\n",
    "\n",
    "# how many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = 120000\n",
    "# Percentile of Padding to use with Tokenized words\n",
    "pad_percentile = 50\n",
    "\n",
    "# Use Keras Tokenizer\n",
    "use_tokenizer = False\n",
    "# Use TF IDF Vectorizer\n",
    "use_tf_idf = True\n",
    "\n",
    "# Text Column name\n",
    "text_col = 'lower_text'\n",
    "\n",
    "# How many PCA Components to consider for modelling\n",
    "pca_components = 700\n",
    "\n",
    "# Seed\n",
    "numpy_seed = 478\n",
    "seed = 7\n",
    "# Number of Splits\n",
    "n_splits = 10\n",
    "# Scoring Criteria\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "# if Debug is True, loads Subsequent Dataframes from Disk.\n",
    "debug = False\n",
    "\n",
    "train_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test DF\n",
    "\n",
    "Create Training and Testing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Automate it to take path based on OS\n",
    "#ROOT_DIR = r'/Users/shabhushan/Desktop/python/python-code/dataset/notracking/participants' # Mac Directory Path\n",
    "ROOT_DIR = r'/home/shashi/Desktop/projects/python-code/dataset/notracking/participants' # Linux Directory Path\n",
    "\n",
    "TRAIN_LABELS = os.path.join(ROOT_DIR, r'train', r'labels', r'labels.csv')\n",
    "TRAIN_TEXT = os.path.join(ROOT_DIR, r'train', r'extracted_data', r'extract_combined.csv')\n",
    "TEST_TEXT = os.path.join(ROOT_DIR, r'test', r'extracted_data', r'extract_combined.csv')\n",
    "\n",
    "SUB = os.path.join(ROOT_DIR, r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in training and testing data\n",
    "# one dataframe for labels another for text features\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target variable to number\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n",
    "\n",
    "Some Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_from_word_list(lst):\n",
    "    temp_set_list = [set(nltk.word_tokenize(words)) for words in lst]\n",
    "\n",
    "    return reduce(lambda x, y: {*x, *y}, temp_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(df):\n",
    "    tokenized_words = [nltk.word_tokenize(words) for words in df]\n",
    "    words_list = reduce(lambda x, y: [*x, *y], tokenized_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorizer.fit_transform(words_list)\n",
    "\n",
    "    return pd.DataFrame(vectorizer.vocabulary_.items(), columns=['Text', 'Frequency']).sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(train_df, test_df):\n",
    "    \"\"\"\n",
    "        Get the TF IDF Vector representation for Train and Test data frame\n",
    "        \n",
    "        Creates the TF-IDF Vector, Fit on Training data and transform both training and test data frames\n",
    "        Also, returns feature names for creating a Dataframe later\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "    X_train = vectorizer.fit_transform(train_df)\n",
    "    \n",
    "    X_test = vectorizer.transform(test_df)\n",
    "\n",
    "    return X_train, X_test, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    \"\"\"\n",
    "        Break a Sentence into words, remove Stop words keeping only alphabet and numbers, remove\n",
    "        punctuations, comma etc. and at the end Lemmatize the words.\n",
    "    \"\"\"\n",
    "    tokenized_words = nltk.word_tokenize(words)\n",
    "    \n",
    "    # Remove Stop words\n",
    "    words = [word for word in tokenized_words if word.lower() not in stop_words and word.lower() in english_corpus]\n",
    "    \n",
    "    # Remove Digits, Keep only Alpha Numeric words\n",
    "    words = [word for word in words if word.isalnum() and not word.isdigit()]\n",
    "\n",
    "    # Lemmatize based on root word\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    return ' '.join([lemma.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "        Add few extra features to the Data Frame\n",
    "        \n",
    "        text: convert to string\n",
    "        lower_text: lowers the text\n",
    "        total_length: length of the document\n",
    "        capitals: number of capitals in document\n",
    "        caps_vs_length: ratio of capital words to total length\n",
    "        num_words: number of words in document.\n",
    "        num_unique_words: number of unique words in document\n",
    "        words_vs_unique: number of unique words in document\n",
    "        document_type: whether the docoment is pdf, doc or docx\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x:str(x))\n",
    "    df[\"lower_text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    df['total_length'] = df['lower_text'].apply(len)\n",
    "    df['capitals'] = df['lower_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['lower_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
    "    df['document_type'] = df['document_name'].apply(lambda val: val.split(\".\")[-1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_top_words_list(df):\n",
    "    word_list = []\n",
    "\n",
    "    df['lower_text'].map(lambda row: word_list.extend(row.split()))\n",
    "\n",
    "    counter_df = pd.DataFrame.from_dict(Counter(word_list), orient='index').reset_index()\n",
    "\n",
    "    counter_df.columns = ['word', 'frequency']\n",
    "\n",
    "    return counter_df.sort_values(by = 'frequency', ascending = False)\n",
    "\n",
    "def get_top_words(df):\n",
    "    # Segregated Positive and Negative classes\n",
    "    top_counter_df_no = get_top_words_list(df[df.is_fitara == 0])\n",
    "    top_counter_df_yes = get_top_words_list(df[df.is_fitara == 1])\n",
    "    \n",
    "    # Fetch Words in Negative class, which are not in Positive class\n",
    "    exclusive_no = set(top_counter_df_no['word'].values) - set(top_counter_df_yes['word'].values)\n",
    "    # Fetch Words in Positive class, which are not in Negative class\n",
    "    exclusive_yes = set(top_counter_df_yes['word'].values) - set(top_counter_df_no['word'].values)\n",
    "    \n",
    "    # English Words Corpus\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    # Keep valid english words only, remove random words\n",
    "    exclusive_no = [word for word in exclusive_no if word in english_words]\n",
    "    exclusive_yes = [word for word in exclusive_yes if word in english_words]\n",
    "    \n",
    "    # Get the Frequency of corresponding words from Original Dataframe\n",
    "    exclusive_no_df = top_counter_df_no[top_counter_df_no['word'].isin(exclusive_no)]\n",
    "    exclusive_yes_df = top_counter_df_yes[top_counter_df_yes['word'].isin(exclusive_yes)]\n",
    "    \n",
    "    return exclusive_no_df, exclusive_yes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentile_features(df):\n",
    "    df['percentile_99'] = df[text_col].apply(get_positive_words_99)\n",
    "    df['percentile_95'] = df[text_col].apply(get_positive_words_95)\n",
    "    df['percentile_90'] = df[text_col].apply(get_positive_words_90)\n",
    "    df['percentile_85'] = df[text_col].apply(get_positive_words_85)\n",
    "    df['percentile_80'] = df[text_col].apply(get_positive_words_80)\n",
    "    df['percentile_75'] = df[text_col].apply(get_positive_words_75)\n",
    "    df['percentile_70'] = df[text_col].apply(get_positive_words_70)\n",
    "    df['percentile_65'] = df[text_col].apply(get_positive_words_65)\n",
    "    df['percentile_60'] = df[text_col].apply(get_positive_words_60)\n",
    "    df['percentile_55'] = df[text_col].apply(get_positive_words_55)\n",
    "    df['percentile_50'] = df[text_col].apply(get_positive_words_50)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Basic Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tokenization and Lemmatization\n",
    "\n",
    "First, we need to remove the stop words, punctuation characters and all other special characters from the text.\n",
    "Then, we need to lemmatize the word to it's root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# English Stop Words list\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "english_corpus = set(nltk.corpus.words.words())\n",
    "\n",
    "# Create a Tag Dictionary, Default tag is Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "# Get a Lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 44s, sys: 129 ms, total: 3min 45s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Caution: will take time to lemmatize whole Data\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since, we have fetched document type from name and lower case text from text, we could safely remove these two columns\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "test_df.to_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_df = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Primilinary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Positive and Negative classes are size 71% and 29% respectively. Hence, no severe class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.713089\n",
       "1    0.286911\n",
       "Name: is_fitara, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# confirm class distribution\n",
    "# is_fitara - yes: ~29%; no: ~71%\n",
    "train_df['is_fitara'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 25 words \n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_no_df.head(25))\n",
    "#plt.plot()\n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_yes_df.head(25))\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation\n",
    "If ablation is true, use only certain percentage of data for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation:\n",
    "    train_df_no = train_df[train_df.is_fitara == 0]\n",
    "    train_df_yes = train_df[train_df.is_fitara == 1]\n",
    "\n",
    "    # Get 15% of total Records for Ablation\n",
    "    train_df_no_ablation = train_df_no.loc[0:int(len(train_df_no) * ablation_ratio)]\n",
    "    train_df_yes_ablation = train_df_yes.loc[0:int(len(train_df_yes) * ablation_ratio)]\n",
    "    \n",
    "    # Shuffle rows and reset index\n",
    "    train_df = pd.concat([train_df_yes_ablation, train_df_no_ablation]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Split into Test and training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y = train_df['is_fitara']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF\n",
    "Create TF IDF Vector representation for Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tf_idf:\n",
    "    X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "    X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "    X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "    X_train.drop([text_col], axis=1, inplace=True)\n",
    "    X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "if use_tokenizer:\n",
    "    cols = X_train.columns\n",
    "elif use_tf_idf:\n",
    "    cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'), index_col='Unnamed: 0')\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'), index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aa', 'abandon', 'abase', 'abate', 'abatement', 'abattoir', 'abb',\n",
      "       'abbreviation', 'abdominal', 'aberrant',\n",
      "       ...\n",
      "       'zero', 'zimbabwe', 'zinc', 'zip', 'zone', 'zoning', 'zoo', 'zoom',\n",
      "       'zoonotic', 'zoster'],\n",
      "      dtype='object', length=10585)\n",
      "Variance Explained by Model is 0.9930248091267025\n"
     ]
    }
   ],
   "source": [
    "standard_columns = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique', \n",
    "        'doc', 'docx', 'pdf']\n",
    "    #, 'percentile_99', 'percentile_95', 'percentile_90',\n",
    "       #'percentile_85', 'percentile_80', 'percentile_75', 'percentile_70',\n",
    "       #'percentile_65', 'percentile_60', 'percentile_55', 'percentile_50']\n",
    "\n",
    "cols = X_train.columns[len(standard_columns):]\n",
    "\n",
    "print(cols)\n",
    "pca = PCA(n_components = 500)\n",
    "\n",
    "pca.fit(X_train[cols])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[cols]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[cols]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Columns\n",
    "X_train.drop(cols, axis=1, inplace=True)\n",
    "X_test.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pca_2.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_pca_2.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pca.csv'))\n",
    "y_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_label_pca.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_label_pca.csv'), header=None, index_col='Unnamed: 0')\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_label_pca.csv'), header=None, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "First Try fitting a Simple Neural Network for benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check cross validation score for different algorithms on training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB :: -0.396334 ( +- 0.074692) \n",
      " Log Loss : 0.308397\n",
      " Accuracy : 86.910000\n",
      "\n",
      "GBC :: -0.380622 ( +- 0.071621) \n",
      " Log Loss : 0.352752\n",
      " Accuracy : 85.860000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "estimators = []\n",
    "\n",
    "#estimators.append(('LR', LogisticRegression()))\n",
    "#estimators.append(('NB', BernoulliNB(alpha=.01)))\n",
    "#estimators.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#estimators.append(('KNN', KNeighborsClassifier()))\n",
    "#estimators.append(('TREE', DecisionTreeClassifier()))\n",
    "#estimators.append(('CART', RandomForestClassifier(n_estimators=100)))\n",
    "#estimators.append(('NB', GaussianNB()))\n",
    "#estimators.append(('SVM', SVC(probability=True)))\n",
    "estimators.append(('XGB', XGBClassifier()))\n",
    "estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    #cv_results = model_selection.cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    log_loss = metrics.log_loss(y_test, model.predict_proba(X_test))\n",
    "    accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n Log Loss : %f\\n Accuracy : %f\\n\" % (name, cv_results.mean(), cv_results.std(), log_loss, round(accuracy*100, 2))\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31238222213878786"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(estimators[0][1].predict_proba(X_test)).iloc[:, 1].apply(lambda val: 0.99 if val > 0.95 else val)\n",
    "\n",
    "#metrics.log_loss(y_test, df)\n",
    "#df.apply(lambda val: 0.01)\n",
    "#df = pd.DataFrame(estimators[0][1].predict_proba(X_test)).iloc[:, 1].apply(lambda val : 0.01 if val < 0.05 else val)\n",
    "df = df.apply(lambda val : 0.01 if val < 0.05 else val)\n",
    "\n",
    "metrics.log_loss(y_test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction No</th>\n",
       "      <th>Prediction Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>131</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction No  Prediction Yes\n",
       "Actual No             131               8\n",
       "Actual Yes             17              35"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, estimators[0][1].predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n",
    "\n",
    "#print(metrics.accuracy_score(y_test.values, estimators[2][1].predict(X_test)))\n",
    "#print(metrics.log_loss(y_test.values, estimators[4][1].predict_proba(X_test)))\n",
    "#math.exp(-metrics.log_loss(y_test.values, estimators[5][1].predict_proba(X_test)))\n",
    "\n",
    "#y_pred = estimators[2][1].predict(X_test)\n",
    "#metrics.roc_curve(y_test.values, y_pred)\n",
    "#estimators[2][1].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: -0.380267 (+/- 0.070440)\n",
      "0.3244317985322947\n",
      "0.8691099476439791\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction No</th>\n",
       "      <th>Prediction Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>133</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction No  Prediction Yes\n",
       "Actual No             133               6\n",
       "Actual Yes             19              33"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(ensemble, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(metrics.log_loss(y_test, ensemble.predict_proba(X_test)))\n",
    "print(metrics.accuracy_score(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, ensemble.predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32426554759986526"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ensemble.predict_proba(X_test)).iloc[:, 1].apply(lambda val: 0.99 if val > 0.95 else val)\n",
    "\n",
    "#metrics.log_loss(y_test, df)\n",
    "#df.apply(lambda val: 0.01)\n",
    "#df = pd.DataFrame(estimators[0][1].predict_proba(X_test)).iloc[:, 1].apply(lambda val : 0.01 if val < 0.05 else val)\n",
    "#df = df.apply(lambda val : 0.01 if val < 0.05 else val)\n",
    "\n",
    "metrics.log_loss(y_test, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 240 ms, total: 3min 55s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df\n",
    "\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)\n",
    "\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "X_train = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y_train = train_df['is_fitara']\n",
    "\n",
    "X_test = test_df.drop(['document_name'], axis=1)\n",
    "\n",
    "X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "X_train.drop([text_col], axis=1, inplace=True)\n",
    "X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'final_train_X.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'final_train_label_X.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'final_test_X.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Features\n",
    "cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique', \n",
    "'document_type_doc', 'document_type_docx', 'document_type_pdf']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Explained by Model is 0.998294925172346\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "columns = X_train.columns[len(cols):]\n",
    "\n",
    "pca = PCA(n_components = pca_components)\n",
    "\n",
    "pca.fit(X_train[columns])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[columns]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[columns]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")\n",
    "\n",
    "# Remove Old Columns\n",
    "X_train.drop(columns, axis=1, inplace=True)\n",
    "X_test.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB :: -0.356439 ( +- 0.073929) \n",
      "\n",
      "GBC :: -0.369761 ( +- 0.067399) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEVCAYAAADtmeJyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAU00lEQVR4nO3dfbRldX3f8feHjqCpoHMzOAxCmGUlLRpxWk5UEjA2DqlpXYJrdanpRAZWlRpcsUnqw0S00qopJhKfULImrMKgEjW0CEaNwCRKtGC9Y0cYmuiAFhmYgSuMAj6A4rd/nH2T4/XcmXvn3Ie59/d+rXXW3Xv/fvvs7z6z53zO/u3zkKpCktSuQxa7AEnS4jIIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBoTiW5LMnb5um+NyS5dh/tz0uyaz62vdQleWOSSxa7Dh2cDAIdkCSfTbI3yWELtc2q+nBV/dpADZXkqQu1/fS9JsmOJN9NsivJnyd5xkLVcKCq6g+q6hWLXYcOTgaBZi3JWuBUoIAXLdA2VyzEdvbjPcB/BF4DjAE/D3wc+DeLWdT+HCSPnQ5iBoEOxJnATcBlwMZ9dUzy+iS7k9yd5BWDr+KTPCHJ5UkmktyR5E1JDunazkryhSTvSnIfcH637PNd+w3dJr6S5KEkLx3Y5n9Kcm+33bMHll+W5ANJPt2t84UkRyV5d3d283dJ/vk0+3E88GrgN6rqr6rq4ar6XneWcsEs9+fbSb6e5Je65Xd29W6cUuufJLkuyYNJPpfkuIH293TrPZBkW5JTB9rOT3Jlkg8leQA4q1v2oa79sV3bfV0tX0qyums7Osk1Se5PcluSV0653491+/hgkluT9Pb176+lwSDQgTgT+HB3+1eTTyJTJXkB8HvAeuCpwPOmdHkf8ATgKcCvdPd79kD7s4GvA6uBtw+uWFXP7SafWVWPr6qPdvNHdff5ZODfA+9PsnJg1ZcAbwJWAQ8DNwJf7uavBP54mn1+PrCrqv73NO0z3Z+bgZ8FrgA+Avwi/cfmN4GLkjx+oP8G4K1dbdvpP96TvgSso39mcgXw50keO9B+erc/T5yyHvTD+wnAsV0trwK+37V9BNgFHA38W+APkvzqwLov6vo8EbgGuGgfj4eWCINAs5LkFOA44GNVtQ24Hfh303R/CXBpVd1aVd8Dzh+4n38EvAz4/ap6sKr+H3Ah8PKB9e+uqvdV1Y+q6vvMzA+B/1pVP6yqTwEPAf90oP2qqtpWVT8ArgJ+UFWXV9WjwEeBoWcE9J8wd0+30Rnuzzeq6tKBbR3b1fpwVV0LPEI/FCZ9sqpuqKqHgfOAk5McC1BVH6qq+7rH5kLgsCn7eWNVfbyqfjzksfthtz9PrapHu8fjge6+fxl4Q1X9oKq2A5fQD7RJn6+qT3X78EHgmdM9Jlo6DALN1kbg2qr6Vjd/BdMPDx0N3DkwPzi9CngMcMfAsjvov5If1n+m7quqHw3Mfw8YfJV9z8D094fMD/b9ifsF1uxjuzPZn6nboqr2tf2/3/+qegi4n/5jSpLXJvnbJN9J8m36r/BXDVt3iA8CnwE+0g3Z/WGSx3T3fX9VPbiPfdgzMP094LFeg1j6DALNWJLH0X+V/ytJ9iTZA/wu8Mwkw14Z7gaOGZg/dmD6W/RfmR43sOzngLsG5g+mr8bdChyzjzHxmezPbP3949UNGY0Bd3fXA15P/99iZVU9EfgOkIF1p33surOl/1JVTwN+CXgh/Vf9dwNjSQ6fw33QEmAQaDbOAB4FnkZ/fHodcALwN/zk8MGkjwFnJzkhyc8Ab55s6IYWPga8Pcnh3YXQ3wM+NIt67qE/Hj/vqmon8AHgz9L/vMKh3UXXlyXZNEf7M9W/TnJKkkPpXyu4qaruBA4HfgRMACuS/GfgiJneaZJ/meQZ3XDWA/QD7Mfdff8v4L91+3Yi/esso+yDlgCDQLOxkf6Y/zeras/kjf4Fww1Thwiq6tPAe4G/Bm6j/04j6F+kBfht4Lv0Lwh/nv4w03+fRT3nA1u6d7685AD3aTZeQ39f3w98m/71kRcDn+jaR92fqa4A3kJ/SOgk+heUoT+s85fA1+gP3fyA2Q2jHUX/QvIDwN8Cn6M/XATwG8Ba+mcHVwFvqarrR9gHLQHxh2m0UJKcAOwADpsyjq8pklxG/11Kb1rsWrT8eUageZXkxUkO697C+Q7gE4aAdHAxCDTf/gNwL/1hlEeB31rcciRN5dCQJDXOMwJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1LgV++9y8Fm1alWtXbt2scuQpCVl27Zt36qqI6cuX5JBsHbtWsbHxxe7DElaUpLcMWy5Q0OS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxi3JD5Rp9pIc0HpVNceVSDrYGASN2NcTehKf8KWGOTQkSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJatxIQZBkLMl1SXZ2f1cO6bMuyY1Jbk1yc5KXDrRdluQbSbZ3t3Wj1CNJmr1Rzwg2AVur6nhgazc/1feAM6vq6cALgHcneeJA++uqal132z5iPZKkWRo1CE4HtnTTW4Azpnaoqq9V1c5u+m7gXuDIEbcrSZojowbB6qra3U3vAVbvq3OSZwGHArcPLH57N2T0riSH7WPdc5KMJxmfmJgYsWxJ0qT9BkGS65PsGHI7fbBf9b+1bNpvLkuyBvggcHZV/bhb/PvAPwN+ERgD3jDd+lW1uap6VdU78khPKCRpruz320erav10bUnuSbKmqnZ3T/T3TtPvCOCTwHlVddPAfU+eTTyc5FLgtbOqXpI0slGHhq4BNnbTG4Grp3ZIcihwFXB5VV05pW1N9zf0ry/sGLEeSdIsjRoEFwCnJdkJrO/mSdJLcknX5yXAc4GzhrxN9MNJbgFuAVYBbxuxHknSLGUp/iBJr9er8fHxxS5j2fCHaaQ2JNlWVb2py/1ksSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1Lj9fg21JM23/hcQz47fjzV3DAJJi266J3W/EHFhODQkSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxIwdBkrEk1yXZ2f1dOaTPcUm+nGR7kluTvGqg7aQktyS5Lcl7cyA/XipJOmBzcUawCdhaVccDW7v5qXYDJ1fVOuDZwKYkR3dtFwOvBI7vbi+Yg5okSTM0F0FwOrClm94CnDG1Q1U9UlUPd7OHTW43yRrgiKq6qfq/UH35sPUlSfNnLoJgdVXt7qb3AKuHdUpybJKbgTuBd1TV3cCTgV0D3XZ1y4atf06S8STjExMTc1C2JAlgxUw6JbkeOGpI03mDM1VVSWrYfVTVncCJ3ZDQx5NcOZtCq2ozsBmg1+sN3YZgbGyMvXv3znq92VyaWblyJffff/+styHp4DSjIKiq9dO1JbknyZqq2t0N9dy7n/u6O8kO4FTgC8AxA83HAHfNpCYNt3fvXvqjbPPH6/nS8jIXQ0PXABu76Y3A1VM7JDkmyeO66ZXAKcBXuyGlB5I8p3u30JnD1pckzZ+5CIILgNOS7ATWd/Mk6SW5pOtzAvDFJF8BPge8s6pu6drOBS4BbgNuBz49BzVJkmYo8z2MMB96vV6Nj48vdhkHpSQLMjS0FI8bLT0ea3Mrybaq6k1d7ieLJalxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQtCDGxsZIMqsbMKv+Y2Nji7yXS9OKxS5AUhv27t27IL+nrdnzjECSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWrcSEGQZCzJdUl2dn9XDulzXJIvJ9me5NYkrxpo+2ySr3Zt25M8aZR6JEmzN+oZwSZga1UdD2zt5qfaDZxcVeuAZwObkhw90L6hqtZ1t3tHrEeSNEujBsHpwJZuegtwxtQOVfVIVT3czR42B9uUJM2hUZ+UV1fV7m56D7B6WKckxya5GbgTeEdV3T3QfGk3LPTm7OM7ZJOck2Q8yfjExMSIZUuSJu03CJJcn2THkNvpg/2q/0XjQ79svKrurKoTgacCG5NMBsaGqnoGcGp3e/l0dVTV5qrqVVXvyCOPnOHuSZL2Z78/TFNV66drS3JPkjVVtTvJGmCfY/xVdXeSHfSf9K+sqru65Q8muQJ4FnD5rPZAkjSSUYeGrgE2dtMbgaundkhyTJLHddMrgVOAryZZkWRVt/wxwAuBHSPWI0mapVGD4ALgtCQ7gfXdPEl6SS7p+pwAfDHJV4DPAe+sqlvoXzj+THftYDtwF/CnI9YjSZqlkX6zuKruA54/ZPk48Ipu+jrgxCF9vgucNMr2JUmj862cktQ4g0CSGmcQSFLjDAJJatxIF4t18Km3HAHnP2H+tyHNksfmwSv9DwQvLb1er8bHxxe7jINSEub733QhtqHlx2Nz8SXZVlW9qcsdGpKkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxo0cBEnGklyXZGf3d+U++h6RZFeSiwaWnZTkliS3JXlvkoxakyRp5ubijGATsLWqjge2dvPTeStww5RlFwOvBI7vbi+Yg5qalmRebytXTpv1kpaguQiC04Et3fQW4IxhnZKcBKwGrh1YtgY4oqpuqqoCLp9ufc1MVc36Ntv17r///kXeS0lzaS6CYHVV7e6m99B/sv8JSQ4BLgReO6XpycCugfld3bKfkuScJONJxicmJkavWpIEwIqZdEpyPXDUkKbzBmeqqpLUkH7nAp+qql0HegmgqjYDmwF6vd6wbUiSDsCMgqCq1k/XluSeJGuqanc31HPvkG4nA6cmORd4PHBokoeA9wDHDPQ7BrhrxtVLWlLm+70gXr86MDMKgv24BtgIXND9vXpqh6raMDmd5CygV1WbuvkHkjwH+CJwJvC+OahJ0kFm8nrUbCQ5oPU0O3NxjeAC4LQkO4H13TxJekkumcH65wKXALcBtwOfnoOaJEkzlKWYtr1er8bHxxe7jGXDV106WHlszq0k26qqN3W5nyyWpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxIwVBkrEk1yXZ2f1duY++RyTZleSigWWfTfLVJNu725NGqUeSNHujnhFsArZW1fHA1m5+Om8FbhiyfENVretu945YjyRplkYNgtOBLd30FuCMYZ2SnASsBq4dcXuSpDk2ahCsrqrd3fQe+k/2PyHJIcCFwGunuY9Lu2GhNyfJdBtKck6S8STjExMTI5YtSZq0Yn8dklwPHDWk6bzBmaqqJDWk37nAp6pq15Dn+Q1VdVeSw4H/AbwcuHxYHVW1GdgM0Ov1hm1HknQA9hsEVbV+urYk9yRZU1W7k6wBho3xnwycmuRc4PHAoUkeqqpNVXVXt40Hk1wBPItpgkCSND9GHRq6BtjYTW8Erp7aoao2VNXPVdVa+sNDl1fVpiQrkqwCSPIY4IXAjhHrkSTN0qhBcAFwWpKdwPpuniS9JJfsZ93DgM8kuRnYDtwF/OmI9UiSZilVS2+4vdfr1fj4+GKXsWwkYSkeB1r+PDbnVpJtVdWbutxPFktS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4/f5UpZaHIb8XPaN2vwteWv4Mgkb4hC5pOg4NSVLjDAJJapxDQ5IW3b6uYXn9av4ZBJIWnU/qi8uhIUlqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjshQ/yJFkArhjsetYRlYB31rsIqQhPDbn1nFVdeTUhUsyCDS3koxXVW+x65Cm8thcGA4NSVLjDAJJapxBIIDNi12ANA2PzQXgNQJJapxnBJLUOINgmUpybJJvJBnr5ld282uTHJ/kL5LcnmRbkr9O8tyu31lJJpJsT3JrkiuT/Mzi7o2WoySrk1yR5OvdcXhjkhcneV6S73TH4M1Jrk/ypIH1fj3JeJL/m+T/JLlwMfdjOTAIlqmquhO4GLigW3QB/fHWPcAngc1V9U+q6iTgt4GnDKz+0apaV1VPBx4BXrpwlasF6f/s2MeBG6rqKd1x+DLgmK7L33TH4InAl4BXd+v9AnAR8JtV9TSgB9y24DuwzBgEy9u7gOck+R3gFOCdwAbgxqq6ZrJTVe2oqsumrpxkBfCPgb0LU64a8qvAI1X1J5MLquqOqnrfYKcuMA7nH47B1wNvr6q/69Z5tKouXqCaly1/qnIZq6ofJnkd8JfAr3XzTwe+vJ9VX5rkFGAN8DXgE/Ncqtqzv+Pw1CTbgZ8Fvgu8sVv+C4BDQXPMM4Ll79eB3fT/A/2UJFcl2ZHkfw4s/mhVrQOOAm4BXjf/ZaplSd6f5CtJvtQtmhwaOha4FPjDRSxv2TMIlrEk64DTgOcAv5tkDXAr8C8m+1TVi4GzgLGp61f/vcWfAJ67EPWqKVOPw1cDzwd+6ntwgGv4h2PwVuCkea+uMQbBMtWNrV4M/E5VfRP4I/rXCK4AfjnJiwa67+tdQacAt89boWrVXwGPTfJbA8umOw4Hj8E/At6Y5OcBkhyS5FXzV2YbvEawfL0S+GZVXdfNfwA4G3gW8ELgj5O8G7gHeBB428C6k9cIDgF20T9jkOZMVVWSM4B3JXk9MEH/WsAbui6T1wgCfAd4Rbfezd2bH/6se1tzAX+x4DuwzPjJYklqnENDktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMb9f2hjNkxewi8OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "\n",
    "\n",
    "test_estimators = []\n",
    "\n",
    "test_estimators.append(('XGB', XGBClassifier()))\n",
    "test_estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in test_estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=6)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   12.6s remaining:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: -0.356462 (+/- 0.069047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.0s finished\n"
     ]
    }
   ],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(test_estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(ensemble, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', verbose=2, n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ensemble.predict_proba(X_test)).iloc[:, 1].apply(lambda val: 0.999 if val > 0.95 else val)\n",
    "\n",
    "#metrics.log_loss(y_test, df)\n",
    "#df.apply(lambda val: 0.01)\n",
    "#df = pd.DataFrame(estimators[0][1].predict_proba(X_test)).iloc[:, 1].apply(lambda val : 0.001 if val < 0.05 else val)\n",
    "#df = df.apply(lambda val : 0.001 if val < 0.05 else val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction on Test DF\n",
    "test_df['pred_fitara'] = round(pd.DataFrame(ensemble.predict_proba(X_test)).iloc[:, 1],3)\n",
    "\n",
    "submission = test_df.loc[:, ['document_name', 'pred_fitara']]\n",
    "\"\"\"\n",
    "submission = pd.merge(\n",
    "    sub,\n",
    "    test_df,  \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ").loc[:, ['document_name', 'pred']]\n",
    "\n",
    "submission.columns = ['document_name', 'pred_fitara']\n",
    "\"\"\"\n",
    "\n",
    "submission.to_csv(os.path.join(ROOT_DIR, r'submission-6-ensemble.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.085910\n",
       "1      0.276123\n",
       "2      0.019887\n",
       "3      0.849187\n",
       "4      0.052043\n",
       "         ...   \n",
       "234    0.061115\n",
       "235    0.558745\n",
       "236    0.157766\n",
       "237    0.047576\n",
       "238    0.183798\n",
       "Name: 1, Length: 239, dtype: float32"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_estimators[0][1].fit(X_train, y_train)\n",
    "#pd.DataFrame(test_estimators[0][1].predict_proba(X_test)).iloc[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
