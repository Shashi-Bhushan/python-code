{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk \n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    " \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Experiment\n",
    "ablation = False\n",
    "ablation_ratio = 0.15\n",
    "\n",
    "# how many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = 120000\n",
    "# Percentile of Padding to use with Tokenized words\n",
    "pad_percentile = 50\n",
    "\n",
    "# Use Keras Tokenizer\n",
    "use_tokenizer = True\n",
    "use_tf_idf = False\n",
    "\n",
    "pca_components = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test DF\n",
    "\n",
    "Create Training and Testing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#ROOT_DIR = r'/Users/shabhushan/Desktop/python/python-code/dataset/notracking/participants'\n",
    "ROOT_DIR = r'/home/shashi/Desktop/projects/python-code/dataset/notracking/participants'\n",
    "TRAIN_LABELS = os.path.join(ROOT_DIR, r'train', r'labels', r'labels.csv')\n",
    "TRAIN_TEXT = os.path.join(ROOT_DIR, r'train', r'extracted_data', r'extract_combined.csv')\n",
    "TEST_TEXT = os.path.join(ROOT_DIR, r'test', r'extracted_data', r'extract_combined.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# read in training and testing data\n",
    "# one dataframe for labels another for text features\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Primilinary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Positive and Negative classes are size 71% and 29% respectively. Hence, no severe class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.713089\n",
       "1    0.286911\n",
       "Name: is_fitara, dtype: float64"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# confirm class distribution\n",
    "# is_fitara - yes: ~29%; no: ~71%\n",
    "train_df['is_fitara'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tokenization and Lemmatization\n",
    "\n",
    "First, we need to remove the stop words, punctuation characters and all other special characters from the text.\n",
    "Then, we need to lemmatize the word to it's root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# default tag is Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "words = train_df.loc[0, 'text']\n",
    "\n",
    "def lemmatize(words):\n",
    "    # Remove Stop words and keep only Alpha Numeric words\n",
    "    words = [word for word in nltk.word_tokenize(words) if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join([lemma.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 39s, sys: 348 ms, total: 5min 40s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lemmatize)\n",
    "test_df['text'] = test_df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['text'] = df['text'].apply(lambda x:str(x))\n",
    "    df[\"lower_text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    df['total_length'] = df['text'].apply(len)\n",
    "    df['capitals'] = df['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
    "    return df\n",
    "\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'), index = False)\n",
    "test_df.to_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['document_name', 'text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions\n",
    "\n",
    "Some Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_from_word_list(lst):\n",
    "    temp_set_list = [set(nltk.word_tokenize(words)) for words in lst]\n",
    "\n",
    "    return reduce(lambda x, y: {*x, *y}, temp_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_no = get_set_from_word_list(train_df_temp)\n",
    "def get_word_frequency(df):\n",
    "    tokenized_words = [nltk.word_tokenize(words) for words in df]\n",
    "    words_list = reduce(lambda x, y: [*x, *y], tokenized_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorizer.fit_transform(words_list)\n",
    "\n",
    "    return pd.DataFrame(vectorizer.vocabulary_.items(), columns=['Text', 'Frequency']).sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(train_df, test_df):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "    X_train = vectorizer.fit_transform(train_df)\n",
    "    \n",
    "    X_test = vectorizer.transform(test_df)\n",
    "\n",
    "    return X_train, X_test, vectorizer.get_feature_names() #pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names()), X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Experiment\n",
    "Experimentation on a Smaller Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation:\n",
    "    train_df_no = train_df[train_df.is_fitara == 0]\n",
    "    train_df_yes = train_df[train_df.is_fitara == 1]\n",
    "\n",
    "    # Get 15% of total Records for Ablation\n",
    "    train_df_no_ablation = train_df_no.loc[0:int(len(train_df_no) * ablation_ratio)]\n",
    "    train_df_yes_ablation = train_df_yes.loc[0:int(len(train_df_yes) * ablation_ratio)]\n",
    "    \n",
    "    # Shuffle rows and reset index\n",
    "    train_df = pd.concat([train_df_yes_ablation, train_df_no_ablation]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['is_fitara'], axis=1)\n",
    "y = train_df['is_fitara']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'lower_text'\n",
    "\n",
    "if use_tokenizer:\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train[text_col]))\n",
    "    X_train[text_col] = tokenizer.texts_to_sequences(X_train[text_col])\n",
    "    X_test[text_col] = tokenizer.texts_to_sequences(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Max Length\n",
    "train_max_len = np.percentile(X_train[text_col].apply(len), pad_percentile)\n",
    "test_max_len = np.percentile(X_test[text_col].apply(len), pad_percentile)\n",
    "\n",
    "max_len = int(test_max_len if train_max_len < test_max_len else train_max_len)\n",
    "\n",
    "# Pad the Sentences\n",
    "X_train_temp = pd.DataFrame(pad_sequences(X_train[text_col], maxlen=max_len), index = X_train.index)\n",
    "X_test_temp = pd.DataFrame(pad_sequences(X_test[text_col], maxlen=max_len), index = X_test.index)\n",
    "\n",
    "# Remove Existing DF\n",
    "X_train.drop([text_col], axis=1, inplace=True)\n",
    "X_test.drop([text_col], axis=1, inplace=True)\n",
    "\n",
    "# Create new DF\n",
    "X_train = pd.concat([X_train_temp, X_train], axis = 1)\n",
    "X_test = pd.concat([X_test_temp, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "cols = X_train.columns\n",
    "#cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'), index = False)\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=25 must be between 0 and min(n_samples, n_features)=2 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-354-8e43a9ec8492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \"\"\"\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-code-gqSjhHCu/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    424\u001b[0m                              \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                              \u001b[0;34m\"svd_solver='full'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                              % (n_components, min(n_samples, n_features)))\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=25 must be between 0 and min(n_samples, n_features)=2 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = pca_components)\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(pca.transform(X_train))\n",
    "X_test = pd.DataFrame(pca.transform(X_test))\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.994748232187376\n",
      "[1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
      " 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1]\n",
      "CPU times: user 11.1 ms, sys: 6.56 ms, total: 17.6 ms\n",
      "Wall time: 14.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X_train, X_test, feature_names = get_tf_idf(X_train_split['text'], X_test_split['text'])\n",
    "\n",
    "# build pipeline\n",
    "vectorizer = TfidfVectorizer()\n",
    "regressor = SVC()\n",
    "\n",
    "pipeline = Pipeline([('Logistic Regression', BernoulliNB(alpha=.01))])\n",
    "\n",
    "# fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(metrics.log_loss(y_test.values, y_pred))\n",
    "print(y_pred)\n",
    "print(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.971449250402792\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1]\n",
      "CPU times: user 16.9 ms, sys: 7.84 ms, total: 24.7 ms\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#X_train_lr, X_test_lr, _ = get_tf_idf(X_train, X_test)\n",
    "#model = SVC(kernel='linear', C=100, probability=True, random_state=32)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(metrics.log_loss(y_test.values, y_pred))\n",
    "print(y_pred)\n",
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      " - 0s - loss: 0.6557 - acc: 0.6075\n",
      "Epoch 2/400\n",
      " - 0s - loss: 0.5881 - acc: 0.7067\n",
      "Epoch 3/400\n",
      " - 0s - loss: 0.5837 - acc: 0.7137\n",
      "Epoch 4/400\n",
      " - 0s - loss: 0.5861 - acc: 0.6997\n",
      "Epoch 5/400\n",
      " - 0s - loss: 0.5816 - acc: 0.7165\n",
      "Epoch 6/400\n",
      " - 0s - loss: 0.5850 - acc: 0.7137\n",
      "Epoch 7/400\n",
      " - 0s - loss: 0.5732 - acc: 0.7151\n",
      "Epoch 8/400\n",
      " - 0s - loss: 0.5740 - acc: 0.7221\n",
      "Epoch 9/400\n",
      " - 0s - loss: 0.5682 - acc: 0.7221\n",
      "Epoch 10/400\n",
      " - 0s - loss: 0.5708 - acc: 0.7291\n",
      "Epoch 11/400\n",
      " - 0s - loss: 0.5635 - acc: 0.7193\n",
      "Epoch 12/400\n",
      " - 0s - loss: 0.5616 - acc: 0.7249\n",
      "Epoch 13/400\n",
      " - 0s - loss: 0.5605 - acc: 0.7207\n",
      "Epoch 14/400\n",
      " - 0s - loss: 0.5620 - acc: 0.7277\n",
      "Epoch 15/400\n",
      " - 0s - loss: 0.5564 - acc: 0.7318\n",
      "Epoch 16/400\n",
      " - 0s - loss: 0.5552 - acc: 0.7374\n",
      "Epoch 17/400\n",
      " - 0s - loss: 0.5439 - acc: 0.7388\n",
      "Epoch 18/400\n",
      " - 0s - loss: 0.5560 - acc: 0.7402\n",
      "Epoch 19/400\n",
      " - 0s - loss: 0.5606 - acc: 0.7416\n",
      "Epoch 20/400\n",
      " - 0s - loss: 0.5593 - acc: 0.7402\n",
      "Epoch 21/400\n",
      " - 0s - loss: 0.5510 - acc: 0.7402\n",
      "Epoch 22/400\n",
      " - 0s - loss: 0.5523 - acc: 0.7486\n",
      "Epoch 23/400\n",
      " - 0s - loss: 0.5439 - acc: 0.7500\n",
      "Epoch 24/400\n",
      " - 0s - loss: 0.5440 - acc: 0.7444\n",
      "Epoch 25/400\n",
      " - 0s - loss: 0.5404 - acc: 0.7500\n",
      "Epoch 26/400\n",
      " - 0s - loss: 0.5305 - acc: 0.7570\n",
      "Epoch 27/400\n",
      " - 0s - loss: 0.5367 - acc: 0.7556\n",
      "Epoch 28/400\n",
      " - 0s - loss: 0.5375 - acc: 0.7570\n",
      "Epoch 29/400\n",
      " - 0s - loss: 0.5372 - acc: 0.7500\n",
      "Epoch 30/400\n",
      " - 0s - loss: 0.5323 - acc: 0.7696\n",
      "Epoch 31/400\n",
      " - 0s - loss: 0.5396 - acc: 0.7514\n",
      "Epoch 32/400\n",
      " - 0s - loss: 0.5362 - acc: 0.7612\n",
      "Epoch 33/400\n",
      " - 0s - loss: 0.5322 - acc: 0.7584\n",
      "Epoch 34/400\n",
      " - 0s - loss: 0.5193 - acc: 0.7570\n",
      "Epoch 35/400\n",
      " - 0s - loss: 0.5272 - acc: 0.7528\n",
      "Epoch 36/400\n",
      " - 0s - loss: 0.5322 - acc: 0.7598\n",
      "Epoch 37/400\n",
      " - 0s - loss: 0.5293 - acc: 0.7626\n",
      "Epoch 38/400\n",
      " - 0s - loss: 0.5238 - acc: 0.7668\n",
      "Epoch 39/400\n",
      " - 0s - loss: 0.5191 - acc: 0.7626\n",
      "Epoch 40/400\n",
      " - 0s - loss: 0.5150 - acc: 0.7723\n",
      "Epoch 41/400\n",
      " - 0s - loss: 0.5302 - acc: 0.7709\n",
      "Epoch 42/400\n",
      " - 0s - loss: 0.5001 - acc: 0.7835\n",
      "Epoch 43/400\n",
      " - 0s - loss: 0.5111 - acc: 0.7765\n",
      "Epoch 44/400\n",
      " - 0s - loss: 0.5195 - acc: 0.7751\n",
      "Epoch 45/400\n",
      " - 0s - loss: 0.5159 - acc: 0.7612\n",
      "Epoch 46/400\n",
      " - 0s - loss: 0.5206 - acc: 0.7654\n",
      "Epoch 47/400\n",
      " - 0s - loss: 0.5269 - acc: 0.7556\n",
      "Epoch 48/400\n",
      " - 0s - loss: 0.5246 - acc: 0.7528\n",
      "Epoch 49/400\n",
      " - 0s - loss: 0.5065 - acc: 0.7696\n",
      "Epoch 50/400\n",
      " - 0s - loss: 0.5093 - acc: 0.7723\n",
      "Epoch 51/400\n",
      " - 0s - loss: 0.5206 - acc: 0.7793\n",
      "Epoch 52/400\n",
      " - 0s - loss: 0.5118 - acc: 0.7779\n",
      "Epoch 53/400\n",
      " - 0s - loss: 0.5162 - acc: 0.7682\n",
      "Epoch 54/400\n",
      " - 0s - loss: 0.5192 - acc: 0.7668\n",
      "Epoch 55/400\n",
      " - 0s - loss: 0.4967 - acc: 0.7807\n",
      "Epoch 56/400\n",
      " - 0s - loss: 0.5152 - acc: 0.7737\n",
      "Epoch 57/400\n",
      " - 0s - loss: 0.5162 - acc: 0.7612\n",
      "Epoch 58/400\n",
      " - 0s - loss: 0.5014 - acc: 0.7793\n",
      "Epoch 59/400\n",
      " - 0s - loss: 0.5113 - acc: 0.7640\n",
      "Epoch 60/400\n",
      " - 0s - loss: 0.5187 - acc: 0.7500\n",
      "Epoch 61/400\n",
      " - 0s - loss: 0.4983 - acc: 0.7765\n",
      "Epoch 62/400\n",
      " - 0s - loss: 0.5137 - acc: 0.7626\n",
      "Epoch 63/400\n",
      " - 0s - loss: 0.4866 - acc: 0.7751\n",
      "Epoch 64/400\n",
      " - 0s - loss: 0.4956 - acc: 0.7793\n",
      "Epoch 65/400\n",
      " - 0s - loss: 0.5019 - acc: 0.7765\n",
      "Epoch 66/400\n",
      " - 0s - loss: 0.5033 - acc: 0.7779\n",
      "Epoch 67/400\n",
      " - 0s - loss: 0.4965 - acc: 0.7723\n",
      "Epoch 68/400\n",
      " - 0s - loss: 0.4879 - acc: 0.7947\n",
      "Epoch 69/400\n",
      " - 0s - loss: 0.5039 - acc: 0.7835\n",
      "Epoch 70/400\n",
      " - 0s - loss: 0.4946 - acc: 0.7835\n",
      "Epoch 71/400\n",
      " - 0s - loss: 0.5083 - acc: 0.7737\n",
      "Epoch 72/400\n",
      " - 0s - loss: 0.4944 - acc: 0.7793\n",
      "Epoch 73/400\n",
      " - 0s - loss: 0.5009 - acc: 0.7723\n",
      "Epoch 74/400\n",
      " - 0s - loss: 0.4822 - acc: 0.7933\n",
      "Epoch 75/400\n",
      " - 0s - loss: 0.4900 - acc: 0.7793\n",
      "Epoch 76/400\n",
      " - 0s - loss: 0.4860 - acc: 0.7849\n",
      "Epoch 77/400\n",
      " - 0s - loss: 0.4888 - acc: 0.7807\n",
      "Epoch 78/400\n",
      " - 0s - loss: 0.4956 - acc: 0.7723\n",
      "Epoch 79/400\n",
      " - 0s - loss: 0.4926 - acc: 0.7723\n",
      "Epoch 80/400\n",
      " - 0s - loss: 0.4765 - acc: 0.7863\n",
      "Epoch 81/400\n",
      " - 0s - loss: 0.4916 - acc: 0.7933\n",
      "Epoch 82/400\n",
      " - 0s - loss: 0.4908 - acc: 0.7793\n",
      "Epoch 83/400\n",
      " - 0s - loss: 0.4833 - acc: 0.7891\n",
      "Epoch 84/400\n",
      " - 0s - loss: 0.4799 - acc: 0.7905\n",
      "Epoch 85/400\n",
      " - 0s - loss: 0.4867 - acc: 0.7905\n",
      "Epoch 86/400\n",
      " - 0s - loss: 0.4873 - acc: 0.7793\n",
      "Epoch 87/400\n",
      " - 0s - loss: 0.4939 - acc: 0.7849\n",
      "Epoch 88/400\n",
      " - 0s - loss: 0.4750 - acc: 0.7877\n",
      "Epoch 89/400\n",
      " - 0s - loss: 0.4912 - acc: 0.7765\n",
      "Epoch 90/400\n",
      " - 0s - loss: 0.4850 - acc: 0.7863\n",
      "Epoch 91/400\n",
      " - 0s - loss: 0.4796 - acc: 0.7751\n",
      "Epoch 92/400\n",
      " - 0s - loss: 0.4806 - acc: 0.7919\n",
      "Epoch 93/400\n",
      " - 0s - loss: 0.4766 - acc: 0.7891\n",
      "Epoch 94/400\n",
      " - 0s - loss: 0.4748 - acc: 0.7961\n",
      "Epoch 95/400\n",
      " - 0s - loss: 0.4818 - acc: 0.7933\n",
      "Epoch 96/400\n",
      " - 0s - loss: 0.4816 - acc: 0.7947\n",
      "Epoch 97/400\n",
      " - 0s - loss: 0.4860 - acc: 0.7863\n",
      "Epoch 98/400\n",
      " - 0s - loss: 0.4864 - acc: 0.7863\n",
      "Epoch 99/400\n",
      " - 0s - loss: 0.4605 - acc: 0.8073\n",
      "Epoch 100/400\n",
      " - 0s - loss: 0.4732 - acc: 0.7975\n",
      "Epoch 101/400\n",
      " - 0s - loss: 0.4676 - acc: 0.7961\n",
      "Epoch 102/400\n",
      " - 0s - loss: 0.4709 - acc: 0.8017\n",
      "Epoch 103/400\n",
      " - 0s - loss: 0.4703 - acc: 0.7905\n",
      "Epoch 104/400\n",
      " - 0s - loss: 0.4645 - acc: 0.7919\n",
      "Epoch 105/400\n",
      " - 0s - loss: 0.4784 - acc: 0.8031\n",
      "Epoch 106/400\n",
      " - 0s - loss: 0.4795 - acc: 0.7807\n",
      "Epoch 107/400\n",
      " - 0s - loss: 0.4661 - acc: 0.8017\n",
      "Epoch 108/400\n",
      " - 0s - loss: 0.4879 - acc: 0.7751\n",
      "Epoch 109/400\n",
      " - 0s - loss: 0.4699 - acc: 0.7905\n",
      "Epoch 110/400\n",
      " - 0s - loss: 0.4653 - acc: 0.7961\n",
      "Epoch 111/400\n",
      " - 0s - loss: 0.4674 - acc: 0.7975\n",
      "Epoch 112/400\n",
      " - 0s - loss: 0.4743 - acc: 0.8045\n",
      "Epoch 113/400\n",
      " - 0s - loss: 0.4741 - acc: 0.7961\n",
      "Epoch 114/400\n",
      " - 0s - loss: 0.4601 - acc: 0.8045\n",
      "Epoch 115/400\n",
      " - 0s - loss: 0.4593 - acc: 0.7947\n",
      "Epoch 116/400\n",
      " - 0s - loss: 0.4612 - acc: 0.8059\n",
      "Epoch 117/400\n",
      " - 0s - loss: 0.4554 - acc: 0.8073\n",
      "Epoch 118/400\n",
      " - 0s - loss: 0.4501 - acc: 0.7947\n",
      "Epoch 119/400\n",
      " - 0s - loss: 0.4808 - acc: 0.7919\n",
      "Epoch 120/400\n",
      " - 0s - loss: 0.4615 - acc: 0.7975\n",
      "Epoch 121/400\n",
      " - 0s - loss: 0.4674 - acc: 0.7919\n",
      "Epoch 122/400\n",
      " - 0s - loss: 0.4579 - acc: 0.7933\n",
      "Epoch 123/400\n",
      " - 0s - loss: 0.4391 - acc: 0.8087\n",
      "Epoch 124/400\n",
      " - 0s - loss: 0.4619 - acc: 0.7961\n",
      "Epoch 125/400\n",
      " - 0s - loss: 0.4644 - acc: 0.7961\n",
      "Epoch 126/400\n",
      " - 0s - loss: 0.4440 - acc: 0.8031\n",
      "Epoch 127/400\n",
      " - 0s - loss: 0.4520 - acc: 0.7989\n",
      "Epoch 128/400\n",
      " - 0s - loss: 0.4634 - acc: 0.7933\n",
      "Epoch 129/400\n",
      " - 0s - loss: 0.4635 - acc: 0.8031\n",
      "Epoch 130/400\n",
      " - 0s - loss: 0.4535 - acc: 0.7933\n",
      "Epoch 131/400\n",
      " - 0s - loss: 0.4618 - acc: 0.7905\n",
      "Epoch 132/400\n",
      " - 0s - loss: 0.4439 - acc: 0.8045\n",
      "Epoch 133/400\n",
      " - 0s - loss: 0.4559 - acc: 0.8003\n",
      "Epoch 134/400\n",
      " - 0s - loss: 0.4665 - acc: 0.7919\n",
      "Epoch 135/400\n",
      " - 0s - loss: 0.4482 - acc: 0.8031\n",
      "Epoch 136/400\n",
      " - 0s - loss: 0.4534 - acc: 0.8198\n",
      "Epoch 137/400\n",
      " - 0s - loss: 0.4663 - acc: 0.7877\n",
      "Epoch 138/400\n",
      " - 0s - loss: 0.4588 - acc: 0.7933\n",
      "Epoch 139/400\n",
      " - 0s - loss: 0.4455 - acc: 0.7933\n",
      "Epoch 140/400\n",
      " - 0s - loss: 0.4390 - acc: 0.8059\n",
      "Epoch 141/400\n",
      " - 0s - loss: 0.4553 - acc: 0.7989\n",
      "Epoch 142/400\n",
      " - 0s - loss: 0.4499 - acc: 0.7947\n",
      "Epoch 143/400\n",
      " - 0s - loss: 0.4375 - acc: 0.8156\n",
      "Epoch 144/400\n",
      " - 0s - loss: 0.4522 - acc: 0.8003\n",
      "Epoch 145/400\n",
      " - 0s - loss: 0.4445 - acc: 0.8156\n",
      "Epoch 146/400\n",
      " - 0s - loss: 0.4366 - acc: 0.8115\n",
      "Epoch 147/400\n",
      " - 0s - loss: 0.4448 - acc: 0.8017\n",
      "Epoch 148/400\n",
      " - 0s - loss: 0.4457 - acc: 0.8087\n",
      "Epoch 149/400\n",
      " - 0s - loss: 0.4529 - acc: 0.8031\n",
      "Epoch 150/400\n",
      " - 0s - loss: 0.4408 - acc: 0.8031\n",
      "Epoch 151/400\n",
      " - 0s - loss: 0.4304 - acc: 0.8240\n",
      "Epoch 152/400\n",
      " - 0s - loss: 0.4416 - acc: 0.8128\n",
      "Epoch 153/400\n",
      " - 0s - loss: 0.4416 - acc: 0.8101\n",
      "Epoch 154/400\n",
      " - 0s - loss: 0.4296 - acc: 0.8059\n",
      "Epoch 155/400\n",
      " - 0s - loss: 0.4451 - acc: 0.8087\n",
      "Epoch 156/400\n",
      " - 0s - loss: 0.4562 - acc: 0.7975\n",
      "Epoch 157/400\n",
      " - 0s - loss: 0.4448 - acc: 0.8115\n",
      "Epoch 158/400\n",
      " - 0s - loss: 0.4520 - acc: 0.8031\n",
      "Epoch 159/400\n",
      " - 0s - loss: 0.4390 - acc: 0.8003\n",
      "Epoch 160/400\n",
      " - 0s - loss: 0.4357 - acc: 0.8003\n",
      "Epoch 161/400\n",
      " - 0s - loss: 0.4355 - acc: 0.8017\n",
      "Epoch 162/400\n",
      " - 0s - loss: 0.4133 - acc: 0.8198\n",
      "Epoch 163/400\n",
      " - 0s - loss: 0.4241 - acc: 0.8198\n",
      "Epoch 164/400\n",
      " - 0s - loss: 0.4290 - acc: 0.8128\n",
      "Epoch 165/400\n",
      " - 0s - loss: 0.4267 - acc: 0.8045\n",
      "Epoch 166/400\n",
      " - 0s - loss: 0.4238 - acc: 0.8184\n",
      "Epoch 167/400\n",
      " - 0s - loss: 0.4303 - acc: 0.8156\n",
      "Epoch 168/400\n",
      " - 0s - loss: 0.4354 - acc: 0.8128\n",
      "Epoch 169/400\n",
      " - 0s - loss: 0.4222 - acc: 0.8184\n",
      "Epoch 170/400\n",
      " - 0s - loss: 0.4361 - acc: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/400\n",
      " - 0s - loss: 0.4401 - acc: 0.8128\n",
      "Epoch 172/400\n",
      " - 0s - loss: 0.4342 - acc: 0.8003\n",
      "Epoch 173/400\n",
      " - 0s - loss: 0.4407 - acc: 0.8017\n",
      "Epoch 174/400\n",
      " - 0s - loss: 0.4308 - acc: 0.8101\n",
      "Epoch 175/400\n",
      " - 0s - loss: 0.4382 - acc: 0.8212\n",
      "Epoch 176/400\n",
      " - 0s - loss: 0.4328 - acc: 0.8128\n",
      "Epoch 177/400\n",
      " - 0s - loss: 0.4157 - acc: 0.8212\n",
      "Epoch 178/400\n",
      " - 0s - loss: 0.4176 - acc: 0.8226\n",
      "Epoch 179/400\n",
      " - 0s - loss: 0.4241 - acc: 0.8184\n",
      "Epoch 180/400\n",
      " - 0s - loss: 0.4124 - acc: 0.8156\n",
      "Epoch 181/400\n",
      " - 0s - loss: 0.4262 - acc: 0.8128\n",
      "Epoch 182/400\n",
      " - 0s - loss: 0.4272 - acc: 0.8156\n",
      "Epoch 183/400\n",
      " - 0s - loss: 0.4191 - acc: 0.8198\n",
      "Epoch 184/400\n",
      " - 0s - loss: 0.4064 - acc: 0.8296\n",
      "Epoch 185/400\n",
      " - 0s - loss: 0.4169 - acc: 0.8198\n",
      "Epoch 186/400\n",
      " - 0s - loss: 0.4183 - acc: 0.8254\n",
      "Epoch 187/400\n",
      " - 0s - loss: 0.4306 - acc: 0.8128\n",
      "Epoch 188/400\n",
      " - 0s - loss: 0.4135 - acc: 0.8142\n",
      "Epoch 189/400\n",
      " - 0s - loss: 0.4308 - acc: 0.8170\n",
      "Epoch 190/400\n",
      " - 0s - loss: 0.4076 - acc: 0.8184\n",
      "Epoch 191/400\n",
      " - 0s - loss: 0.4236 - acc: 0.8156\n",
      "Epoch 192/400\n",
      " - 0s - loss: 0.4185 - acc: 0.8170\n",
      "Epoch 193/400\n",
      " - 0s - loss: 0.4378 - acc: 0.8184\n",
      "Epoch 194/400\n",
      " - 0s - loss: 0.4087 - acc: 0.8254\n",
      "Epoch 195/400\n",
      " - 0s - loss: 0.4109 - acc: 0.8212\n",
      "Epoch 196/400\n",
      " - 0s - loss: 0.4048 - acc: 0.8268\n",
      "Epoch 197/400\n",
      " - 0s - loss: 0.4080 - acc: 0.8226\n",
      "Epoch 198/400\n",
      " - 0s - loss: 0.4139 - acc: 0.8156\n",
      "Epoch 199/400\n",
      " - 0s - loss: 0.4071 - acc: 0.8198\n",
      "Epoch 200/400\n",
      " - 0s - loss: 0.4047 - acc: 0.8212\n",
      "Epoch 201/400\n",
      " - 0s - loss: 0.4048 - acc: 0.8352\n",
      "Epoch 202/400\n",
      " - 0s - loss: 0.3905 - acc: 0.8184\n",
      "Epoch 203/400\n",
      " - 0s - loss: 0.4313 - acc: 0.8156\n",
      "Epoch 204/400\n",
      " - 0s - loss: 0.3967 - acc: 0.8338\n",
      "Epoch 205/400\n",
      " - 0s - loss: 0.4115 - acc: 0.8226\n",
      "Epoch 206/400\n",
      " - 0s - loss: 0.4045 - acc: 0.8268\n",
      "Epoch 207/400\n",
      " - 0s - loss: 0.4098 - acc: 0.8226\n",
      "Epoch 208/400\n",
      " - 0s - loss: 0.3920 - acc: 0.8324\n",
      "Epoch 209/400\n",
      " - 0s - loss: 0.4021 - acc: 0.8310\n",
      "Epoch 210/400\n",
      " - 0s - loss: 0.3993 - acc: 0.8170\n",
      "Epoch 211/400\n",
      " - 0s - loss: 0.4079 - acc: 0.8184\n",
      "Epoch 212/400\n",
      " - 0s - loss: 0.4108 - acc: 0.8254\n",
      "Epoch 213/400\n",
      " - 0s - loss: 0.4215 - acc: 0.8059\n",
      "Epoch 214/400\n",
      " - 0s - loss: 0.4104 - acc: 0.8268\n",
      "Epoch 215/400\n",
      " - 0s - loss: 0.4137 - acc: 0.8087\n",
      "Epoch 216/400\n",
      " - 0s - loss: 0.3983 - acc: 0.8436\n",
      "Epoch 217/400\n",
      " - 0s - loss: 0.3996 - acc: 0.8212\n",
      "Epoch 218/400\n",
      " - 0s - loss: 0.3977 - acc: 0.8268\n",
      "Epoch 219/400\n",
      " - 0s - loss: 0.3926 - acc: 0.8212\n",
      "Epoch 220/400\n",
      " - 0s - loss: 0.3986 - acc: 0.8324\n",
      "Epoch 221/400\n",
      " - 0s - loss: 0.4194 - acc: 0.8156\n",
      "Epoch 222/400\n",
      " - 0s - loss: 0.4262 - acc: 0.8156\n",
      "Epoch 223/400\n",
      " - 0s - loss: 0.4166 - acc: 0.8115\n",
      "Epoch 224/400\n",
      " - 0s - loss: 0.4124 - acc: 0.8212\n",
      "Epoch 225/400\n",
      " - 0s - loss: 0.4051 - acc: 0.8198\n",
      "Epoch 226/400\n",
      " - 0s - loss: 0.3938 - acc: 0.8226\n",
      "Epoch 227/400\n",
      " - 0s - loss: 0.4078 - acc: 0.8268\n",
      "Epoch 228/400\n",
      " - 0s - loss: 0.3935 - acc: 0.8240\n",
      "Epoch 229/400\n",
      " - 0s - loss: 0.4278 - acc: 0.8073\n",
      "Epoch 230/400\n",
      " - 0s - loss: 0.3973 - acc: 0.8212\n",
      "Epoch 231/400\n",
      " - 0s - loss: 0.3964 - acc: 0.8226\n",
      "Epoch 232/400\n",
      " - 0s - loss: 0.3957 - acc: 0.8268\n",
      "Epoch 233/400\n",
      " - 0s - loss: 0.3889 - acc: 0.8380\n",
      "Epoch 234/400\n",
      " - 0s - loss: 0.4002 - acc: 0.8226\n",
      "Epoch 235/400\n",
      " - 0s - loss: 0.3957 - acc: 0.8310\n",
      "Epoch 236/400\n",
      " - 0s - loss: 0.3969 - acc: 0.8310\n",
      "Epoch 237/400\n",
      " - 0s - loss: 0.3992 - acc: 0.8170\n",
      "Epoch 238/400\n",
      " - 0s - loss: 0.3912 - acc: 0.8268\n",
      "Epoch 239/400\n",
      " - 0s - loss: 0.3793 - acc: 0.8422\n",
      "Epoch 240/400\n",
      " - 0s - loss: 0.3913 - acc: 0.8212\n",
      "Epoch 241/400\n",
      " - 0s - loss: 0.3859 - acc: 0.8324\n",
      "Epoch 242/400\n",
      " - 0s - loss: 0.3843 - acc: 0.8310\n",
      "Epoch 243/400\n",
      " - 0s - loss: 0.3963 - acc: 0.8366\n",
      "Epoch 244/400\n",
      " - 0s - loss: 0.3944 - acc: 0.8310\n",
      "Epoch 245/400\n",
      " - 0s - loss: 0.3955 - acc: 0.8184\n",
      "Epoch 246/400\n",
      " - 0s - loss: 0.3874 - acc: 0.8408\n",
      "Epoch 247/400\n",
      " - 0s - loss: 0.3868 - acc: 0.8296\n",
      "Epoch 248/400\n",
      " - 0s - loss: 0.4213 - acc: 0.8184\n",
      "Epoch 249/400\n",
      " - 0s - loss: 0.3779 - acc: 0.8366\n",
      "Epoch 250/400\n",
      " - 0s - loss: 0.3889 - acc: 0.8254\n",
      "Epoch 251/400\n",
      " - 0s - loss: 0.4070 - acc: 0.8212\n",
      "Epoch 252/400\n",
      " - 0s - loss: 0.3872 - acc: 0.8310\n",
      "Epoch 253/400\n",
      " - 0s - loss: 0.4000 - acc: 0.8254\n",
      "Epoch 254/400\n",
      " - 0s - loss: 0.3871 - acc: 0.8310\n",
      "Epoch 255/400\n",
      " - 0s - loss: 0.3804 - acc: 0.8254\n",
      "Epoch 256/400\n",
      " - 0s - loss: 0.3942 - acc: 0.8212\n",
      "Epoch 257/400\n",
      " - 0s - loss: 0.4054 - acc: 0.8170\n",
      "Epoch 258/400\n",
      " - 0s - loss: 0.3962 - acc: 0.8240\n",
      "Epoch 259/400\n",
      " - 0s - loss: 0.3853 - acc: 0.8394\n",
      "Epoch 260/400\n",
      " - 0s - loss: 0.3866 - acc: 0.8212\n",
      "Epoch 261/400\n",
      " - 0s - loss: 0.3809 - acc: 0.8338\n",
      "Epoch 262/400\n",
      " - 0s - loss: 0.3730 - acc: 0.8282\n",
      "Epoch 263/400\n",
      " - 0s - loss: 0.3829 - acc: 0.8394\n",
      "Epoch 264/400\n",
      " - 0s - loss: 0.3919 - acc: 0.8338\n",
      "Epoch 265/400\n",
      " - 0s - loss: 0.3767 - acc: 0.8324\n",
      "Epoch 266/400\n",
      " - 0s - loss: 0.3654 - acc: 0.8450\n",
      "Epoch 267/400\n",
      " - 0s - loss: 0.3709 - acc: 0.8422\n",
      "Epoch 268/400\n",
      " - 0s - loss: 0.3752 - acc: 0.8478\n",
      "Epoch 269/400\n",
      " - 0s - loss: 0.3708 - acc: 0.8380\n",
      "Epoch 270/400\n",
      " - 0s - loss: 0.3739 - acc: 0.8324\n",
      "Epoch 271/400\n",
      " - 0s - loss: 0.3886 - acc: 0.8338\n",
      "Epoch 272/400\n",
      " - 0s - loss: 0.3736 - acc: 0.8436\n",
      "Epoch 273/400\n",
      " - 0s - loss: 0.3640 - acc: 0.8380\n",
      "Epoch 274/400\n",
      " - 0s - loss: 0.3824 - acc: 0.8184\n",
      "Epoch 275/400\n",
      " - 0s - loss: 0.3874 - acc: 0.8450\n",
      "Epoch 276/400\n",
      " - 0s - loss: 0.3684 - acc: 0.8380\n",
      "Epoch 277/400\n",
      " - 0s - loss: 0.3610 - acc: 0.8408\n",
      "Epoch 278/400\n",
      " - 0s - loss: 0.3696 - acc: 0.8450\n",
      "Epoch 279/400\n",
      " - 0s - loss: 0.3845 - acc: 0.8240\n",
      "Epoch 280/400\n",
      " - 0s - loss: 0.3782 - acc: 0.8282\n",
      "Epoch 281/400\n",
      " - 0s - loss: 0.3659 - acc: 0.8422\n",
      "Epoch 282/400\n",
      " - 0s - loss: 0.3748 - acc: 0.8408\n",
      "Epoch 283/400\n",
      " - 0s - loss: 0.3573 - acc: 0.8464\n",
      "Epoch 284/400\n",
      " - 0s - loss: 0.3774 - acc: 0.8380\n",
      "Epoch 285/400\n",
      " - 0s - loss: 0.3732 - acc: 0.8352\n",
      "Epoch 286/400\n",
      " - 0s - loss: 0.3881 - acc: 0.8212\n",
      "Epoch 287/400\n",
      " - 0s - loss: 0.3832 - acc: 0.8324\n",
      "Epoch 288/400\n",
      " - 0s - loss: 0.3690 - acc: 0.8380\n",
      "Epoch 289/400\n",
      " - 0s - loss: 0.3533 - acc: 0.8492\n",
      "Epoch 290/400\n",
      " - 0s - loss: 0.3669 - acc: 0.8422\n",
      "Epoch 291/400\n",
      " - 0s - loss: 0.3595 - acc: 0.8394\n",
      "Epoch 292/400\n",
      " - 0s - loss: 0.3605 - acc: 0.8520\n",
      "Epoch 293/400\n",
      " - 0s - loss: 0.3599 - acc: 0.8450\n",
      "Epoch 294/400\n",
      " - 0s - loss: 0.3692 - acc: 0.8394\n",
      "Epoch 295/400\n",
      " - 0s - loss: 0.3546 - acc: 0.8492\n",
      "Epoch 296/400\n",
      " - 0s - loss: 0.3588 - acc: 0.8575\n",
      "Epoch 297/400\n",
      " - 0s - loss: 0.3626 - acc: 0.8310\n",
      "Epoch 298/400\n",
      " - 0s - loss: 0.3586 - acc: 0.8464\n",
      "Epoch 299/400\n",
      " - 0s - loss: 0.3576 - acc: 0.8296\n",
      "Epoch 300/400\n",
      " - 0s - loss: 0.3605 - acc: 0.8464\n",
      "Epoch 301/400\n",
      " - 0s - loss: 0.3598 - acc: 0.8408\n",
      "Epoch 302/400\n",
      " - 0s - loss: 0.3665 - acc: 0.8478\n",
      "Epoch 303/400\n",
      " - 0s - loss: 0.3801 - acc: 0.8352\n",
      "Epoch 304/400\n",
      " - 0s - loss: 0.3735 - acc: 0.8324\n",
      "Epoch 305/400\n",
      " - 0s - loss: 0.3707 - acc: 0.8366\n",
      "Epoch 306/400\n",
      " - 0s - loss: 0.3614 - acc: 0.8352\n",
      "Epoch 307/400\n",
      " - 0s - loss: 0.3605 - acc: 0.8408\n",
      "Epoch 308/400\n",
      " - 0s - loss: 0.3447 - acc: 0.8492\n",
      "Epoch 309/400\n",
      " - 0s - loss: 0.3655 - acc: 0.8380\n",
      "Epoch 310/400\n",
      " - 0s - loss: 0.3726 - acc: 0.8338\n",
      "Epoch 311/400\n",
      " - 0s - loss: 0.3644 - acc: 0.8478\n",
      "Epoch 312/400\n",
      " - 0s - loss: 0.3531 - acc: 0.8492\n",
      "Epoch 313/400\n",
      " - 0s - loss: 0.3677 - acc: 0.8380\n",
      "Epoch 314/400\n",
      " - 0s - loss: 0.3610 - acc: 0.8450\n",
      "Epoch 315/400\n",
      " - 0s - loss: 0.3603 - acc: 0.8561\n",
      "Epoch 316/400\n",
      " - 0s - loss: 0.3468 - acc: 0.8408\n",
      "Epoch 317/400\n",
      " - 0s - loss: 0.3679 - acc: 0.8450\n",
      "Epoch 318/400\n",
      " - 0s - loss: 0.3481 - acc: 0.8478\n",
      "Epoch 319/400\n",
      " - 0s - loss: 0.3609 - acc: 0.8450\n",
      "Epoch 320/400\n",
      " - 0s - loss: 0.3525 - acc: 0.8450\n",
      "Epoch 321/400\n",
      " - 0s - loss: 0.3621 - acc: 0.8478\n",
      "Epoch 322/400\n",
      " - 0s - loss: 0.3528 - acc: 0.8492\n",
      "Epoch 323/400\n",
      " - 0s - loss: 0.3585 - acc: 0.8310\n",
      "Epoch 324/400\n",
      " - 0s - loss: 0.3610 - acc: 0.8422\n",
      "Epoch 325/400\n",
      " - 0s - loss: 0.3656 - acc: 0.8394\n",
      "Epoch 326/400\n",
      " - 0s - loss: 0.3550 - acc: 0.8478\n",
      "Epoch 327/400\n",
      " - 0s - loss: 0.3711 - acc: 0.8450\n",
      "Epoch 328/400\n",
      " - 0s - loss: 0.3403 - acc: 0.8645\n",
      "Epoch 329/400\n",
      " - 0s - loss: 0.3514 - acc: 0.8436\n",
      "Epoch 330/400\n",
      " - 0s - loss: 0.3550 - acc: 0.8366\n",
      "Epoch 331/400\n",
      " - 0s - loss: 0.3920 - acc: 0.8254\n",
      "Epoch 332/400\n",
      " - 0s - loss: 0.3419 - acc: 0.8380\n",
      "Epoch 333/400\n",
      " - 0s - loss: 0.3598 - acc: 0.8338\n",
      "Epoch 334/400\n",
      " - 0s - loss: 0.3515 - acc: 0.8408\n",
      "Epoch 335/400\n",
      " - 0s - loss: 0.3421 - acc: 0.8478\n",
      "Epoch 336/400\n",
      " - 0s - loss: 0.3333 - acc: 0.8520\n",
      "Epoch 337/400\n",
      " - 0s - loss: 0.3496 - acc: 0.8464\n",
      "Epoch 338/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.3509 - acc: 0.8394\n",
      "Epoch 339/400\n",
      " - 0s - loss: 0.3522 - acc: 0.8436\n",
      "Epoch 340/400\n",
      " - 0s - loss: 0.3548 - acc: 0.8506\n",
      "Epoch 341/400\n",
      " - 0s - loss: 0.3378 - acc: 0.8520\n",
      "Epoch 342/400\n",
      " - 0s - loss: 0.3413 - acc: 0.8422\n",
      "Epoch 343/400\n",
      " - 0s - loss: 0.3400 - acc: 0.8478\n",
      "Epoch 344/400\n",
      " - 0s - loss: 0.3413 - acc: 0.8464\n",
      "Epoch 345/400\n",
      " - 0s - loss: 0.3301 - acc: 0.8631\n",
      "Epoch 346/400\n",
      " - 0s - loss: 0.3445 - acc: 0.8422\n",
      "Epoch 347/400\n",
      " - 0s - loss: 0.3396 - acc: 0.8506\n",
      "Epoch 348/400\n",
      " - 0s - loss: 0.3445 - acc: 0.8464\n",
      "Epoch 349/400\n",
      " - 0s - loss: 0.3462 - acc: 0.8450\n",
      "Epoch 350/400\n",
      " - 0s - loss: 0.3370 - acc: 0.8492\n",
      "Epoch 351/400\n",
      " - 0s - loss: 0.3380 - acc: 0.8506\n",
      "Epoch 352/400\n",
      " - 0s - loss: 0.3539 - acc: 0.8506\n",
      "Epoch 353/400\n",
      " - 0s - loss: 0.3276 - acc: 0.8547\n",
      "Epoch 354/400\n",
      " - 0s - loss: 0.3225 - acc: 0.8575\n",
      "Epoch 355/400\n",
      " - 0s - loss: 0.3368 - acc: 0.8492\n",
      "Epoch 356/400\n",
      " - 0s - loss: 0.3355 - acc: 0.8450\n",
      "Epoch 357/400\n",
      " - 0s - loss: 0.3595 - acc: 0.8394\n",
      "Epoch 358/400\n",
      " - 0s - loss: 0.3334 - acc: 0.8603\n",
      "Epoch 359/400\n",
      " - 0s - loss: 0.3481 - acc: 0.8492\n",
      "Epoch 360/400\n",
      " - 0s - loss: 0.3640 - acc: 0.8436\n",
      "Epoch 361/400\n",
      " - 0s - loss: 0.3475 - acc: 0.8366\n",
      "Epoch 362/400\n",
      " - 0s - loss: 0.3270 - acc: 0.8743\n",
      "Epoch 363/400\n",
      " - 0s - loss: 0.3519 - acc: 0.8436\n",
      "Epoch 364/400\n",
      " - 0s - loss: 0.3274 - acc: 0.8645\n",
      "Epoch 365/400\n",
      " - 0s - loss: 0.3530 - acc: 0.8338\n",
      "Epoch 366/400\n",
      " - 0s - loss: 0.3397 - acc: 0.8520\n",
      "Epoch 367/400\n",
      " - 0s - loss: 0.3418 - acc: 0.8408\n",
      "Epoch 368/400\n",
      " - 0s - loss: 0.3315 - acc: 0.8534\n",
      "Epoch 369/400\n",
      " - 0s - loss: 0.3522 - acc: 0.8394\n",
      "Epoch 370/400\n",
      " - 0s - loss: 0.3531 - acc: 0.8436\n",
      "Epoch 371/400\n",
      " - 0s - loss: 0.3324 - acc: 0.8534\n",
      "Epoch 372/400\n",
      " - 0s - loss: 0.3537 - acc: 0.8506\n",
      "Epoch 373/400\n",
      " - 0s - loss: 0.3313 - acc: 0.8408\n",
      "Epoch 374/400\n",
      " - 0s - loss: 0.3240 - acc: 0.8534\n",
      "Epoch 375/400\n",
      " - 0s - loss: 0.3389 - acc: 0.8492\n",
      "Epoch 376/400\n",
      " - 0s - loss: 0.3306 - acc: 0.8603\n",
      "Epoch 377/400\n",
      " - 0s - loss: 0.3336 - acc: 0.8520\n",
      "Epoch 378/400\n",
      " - 0s - loss: 0.3333 - acc: 0.8575\n",
      "Epoch 379/400\n",
      " - 0s - loss: 0.3458 - acc: 0.8450\n",
      "Epoch 380/400\n",
      " - 0s - loss: 0.3438 - acc: 0.8450\n",
      "Epoch 381/400\n",
      " - 0s - loss: 0.3385 - acc: 0.8492\n",
      "Epoch 382/400\n",
      " - 0s - loss: 0.3409 - acc: 0.8520\n",
      "Epoch 383/400\n",
      " - 0s - loss: 0.3274 - acc: 0.8561\n",
      "Epoch 384/400\n",
      " - 0s - loss: 0.3313 - acc: 0.8534\n",
      "Epoch 385/400\n",
      " - 0s - loss: 0.3387 - acc: 0.8534\n",
      "Epoch 386/400\n",
      " - 0s - loss: 0.3360 - acc: 0.8478\n",
      "Epoch 387/400\n",
      " - 0s - loss: 0.3156 - acc: 0.8617\n",
      "Epoch 388/400\n",
      " - 0s - loss: 0.3228 - acc: 0.8534\n",
      "Epoch 389/400\n",
      " - 0s - loss: 0.3553 - acc: 0.8464\n",
      "Epoch 390/400\n",
      " - 0s - loss: 0.3367 - acc: 0.8575\n",
      "Epoch 391/400\n",
      " - 0s - loss: 0.3302 - acc: 0.8492\n",
      "Epoch 392/400\n",
      " - 0s - loss: 0.3232 - acc: 0.8534\n",
      "Epoch 393/400\n",
      " - 0s - loss: 0.3432 - acc: 0.8492\n",
      "Epoch 394/400\n",
      " - 0s - loss: 0.3243 - acc: 0.8589\n",
      "Epoch 395/400\n",
      " - 0s - loss: 0.3250 - acc: 0.8464\n",
      "Epoch 396/400\n",
      " - 0s - loss: 0.3310 - acc: 0.8506\n",
      "Epoch 397/400\n",
      " - 0s - loss: 0.3352 - acc: 0.8520\n",
      "Epoch 398/400\n",
      " - 0s - loss: 0.3413 - acc: 0.8534\n",
      "Epoch 399/400\n",
      " - 0s - loss: 0.3337 - acc: 0.8589\n",
      "Epoch 400/400\n",
      " - 0s - loss: 0.3207 - acc: 0.8506\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# train model\n",
    "history = model.fit(X_train, to_categorical(y_train, 2), epochs=400, batch_size=10, verbose=2)#, class_weight={1:0.96, 0:0.04})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot metrics\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7756504627864294\n",
      "0.46040420777503815\n",
      "[1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_test.values)\n",
    "#print(pred)\n",
    "#print(pred_proba)\n",
    "\n",
    "num = metrics.log_loss(y_test.values, model.predict(X_test))\n",
    "\n",
    "\n",
    "#math.exp(-num)\n",
    "print(num)\n",
    "print(math.exp(-num))\n",
    "print(y_test.values)\n",
    "print(np.round(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree \n",
    "\n",
    "Random Forest \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terminate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-343-8e797ac3b7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'terminate' is not defined"
     ]
    }
   ],
   "source": [
    "terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time.time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.log_loss(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \"\"\"\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        #if opts.print_top10 and feature_names is not None:\n",
    "        print(\"top 10 keywords per class:\")\n",
    "        for i, label in enumerate(target_names):\n",
    "            top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "            print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "    \"\"\"\n",
    "    #if opts.print_report:\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred))\n",
    "\n",
    "    #if opts.print_cm:\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(max_iter=50, tol=1e-3), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3),\n",
    "         \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "                                       tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
