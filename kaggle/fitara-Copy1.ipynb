{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk \n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    " \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Experiment (takes 15% of Total Data if ablation is true)\n",
    "ablation = False\n",
    "ablation_ratio = 0.6\n",
    "\n",
    "# how many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = 120000\n",
    "# Percentile of Padding to use with Tokenized words\n",
    "pad_percentile = 50\n",
    "\n",
    "# Use Keras Tokenizer\n",
    "use_tokenizer = False\n",
    "# Use TF IDF Vectorizer\n",
    "use_tf_idf = True\n",
    "\n",
    "# Text Column name\n",
    "text_col = 'lower_text'\n",
    "\n",
    "# How many PCA Components to consider for modelling\n",
    "pca_components = 700\n",
    "\n",
    "# Seed\n",
    "numpy_seed = 478\n",
    "seed = 7\n",
    "# Number of Splits\n",
    "n_splits = 10\n",
    "# Scoring Criteria\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "# if Debug is True, loads Subsequent Dataframes from Disk.\n",
    "debug = False\n",
    "\n",
    "train_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test DF\n",
    "\n",
    "Create Training and Testing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Automate it to take path based on OS\n",
    "ROOT_DIR = r'/Users/shabhushan/Desktop/python/python-code/dataset/notracking/participants' # Mac Directory Path\n",
    "#ROOT_DIR = r'/home/shashi/Desktop/projects/python-code/dataset/notracking/participants' # Linux Directory Path\n",
    "\n",
    "TRAIN_LABELS = os.path.join(ROOT_DIR, r'train', r'labels', r'labels.csv')\n",
    "TRAIN_TEXT = os.path.join(ROOT_DIR, r'train', r'extracted_data', r'extract_combined.csv')\n",
    "TEST_TEXT = os.path.join(ROOT_DIR, r'test', r'extracted_data', r'extract_combined.csv')\n",
    "\n",
    "SUB = os.path.join(ROOT_DIR, r'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in training and testing data\n",
    "# one dataframe for labels another for text features\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target variable to number\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n",
    "\n",
    "Some Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_from_word_list(lst):\n",
    "    temp_set_list = [set(nltk.word_tokenize(words)) for words in lst]\n",
    "\n",
    "    return reduce(lambda x, y: {*x, *y}, temp_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(df):\n",
    "    tokenized_words = [nltk.word_tokenize(words) for words in df]\n",
    "    words_list = reduce(lambda x, y: [*x, *y], tokenized_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    vectorizer.fit_transform(words_list)\n",
    "\n",
    "    return pd.DataFrame(vectorizer.vocabulary_.items(), columns=['Text', 'Frequency']).sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(train_df, test_df):\n",
    "    \"\"\"\n",
    "        Get the TF IDF Vector representation for Train and Test data frame\n",
    "        \n",
    "        Creates the TF-IDF Vector, Fit on Training data and transform both training and test data frames\n",
    "        Also, returns feature names for creating a Dataframe later\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "    X_train = vectorizer.fit_transform(train_df)\n",
    "    \n",
    "    X_test = vectorizer.transform(test_df)\n",
    "\n",
    "    return X_train, X_test, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    \"\"\"\n",
    "        Break a Sentence into words, remove Stop words keeping only alphabet and numbers, remove\n",
    "        punctuations, comma etc. and at the end Lemmatize the words.\n",
    "    \"\"\"\n",
    "    tokenized_words = nltk.word_tokenize(words)\n",
    "    \n",
    "    # Remove Stop words\n",
    "    words = [word for word in tokenized_words if word.lower() not in stop_words and word.lower() in corpus]\n",
    "    \n",
    "    # Remove Digits, Keep only Alpha Numeric words\n",
    "    words = [word for word in words if word.isalnum() and not word.isdigit()]\n",
    "\n",
    "    # Lemmatize based on root word\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    return ' '.join([lemma.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "        Add few extra features to the Data Frame\n",
    "        \n",
    "        text: convert to string\n",
    "        lower_text: lowers the text\n",
    "        total_length: length of the document\n",
    "        capitals: number of capitals in document\n",
    "        caps_vs_length: ratio of capital words to total length\n",
    "        num_words: number of words in document.\n",
    "        num_unique_words: number of unique words in document\n",
    "        words_vs_unique: number of unique words in document\n",
    "        document_type: whether the docoment is pdf, doc or docx\n",
    "    \"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x:str(x))\n",
    "    df[\"lower_text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    df['total_length'] = df['lower_text'].apply(len)\n",
    "    df['capitals'] = df['lower_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['lower_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
    "    df['document_type'] = df['document_name'].apply(lambda val: val.split(\".\")[-1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_top_words_list(df):\n",
    "    word_list = []\n",
    "\n",
    "    df['lower_text'].map(lambda row: word_list.extend(row.split()))\n",
    "\n",
    "    counter_df = pd.DataFrame.from_dict(Counter(word_list), orient='index').reset_index()\n",
    "\n",
    "    counter_df.columns = ['word', 'frequency']\n",
    "\n",
    "    return counter_df.sort_values(by = 'frequency', ascending = False)\n",
    "\n",
    "def get_top_words(df):\n",
    "    # Segregated Positive and Negative classes\n",
    "    top_counter_df_no = get_top_words_list(df[df.is_fitara == 0])\n",
    "    top_counter_df_yes = get_top_words_list(df[df.is_fitara == 1])\n",
    "    \n",
    "    # Fetch Words in Negative class, which are not in Positive class\n",
    "    exclusive_no = set(top_counter_df_no['word'].values) - set(top_counter_df_yes['word'].values)\n",
    "    # Fetch Words in Positive class, which are not in Negative class\n",
    "    exclusive_yes = set(top_counter_df_yes['word'].values) - set(top_counter_df_no['word'].values)\n",
    "    \n",
    "    # English Words Corpus\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    # Keep valid english words only, remove random words\n",
    "    exclusive_no = [word for word in exclusive_no if word in english_words]\n",
    "    exclusive_yes = [word for word in exclusive_yes if word in english_words]\n",
    "    \n",
    "    # Get the Frequency of corresponding words from Original Dataframe\n",
    "    exclusive_no_df = top_counter_df_no[top_counter_df_no['word'].isin(exclusive_no)]\n",
    "    exclusive_yes_df = top_counter_df_yes[top_counter_df_yes['word'].isin(exclusive_yes)]\n",
    "    \n",
    "    return exclusive_no_df, exclusive_yes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentile_features(df):\n",
    "    df['percentile_99'] = df[text_col].apply(get_positive_words_99)\n",
    "    df['percentile_95'] = df[text_col].apply(get_positive_words_95)\n",
    "    df['percentile_90'] = df[text_col].apply(get_positive_words_90)\n",
    "    df['percentile_85'] = df[text_col].apply(get_positive_words_85)\n",
    "    df['percentile_80'] = df[text_col].apply(get_positive_words_80)\n",
    "    df['percentile_75'] = df[text_col].apply(get_positive_words_75)\n",
    "    df['percentile_70'] = df[text_col].apply(get_positive_words_70)\n",
    "    df['percentile_65'] = df[text_col].apply(get_positive_words_65)\n",
    "    df['percentile_60'] = df[text_col].apply(get_positive_words_60)\n",
    "    df['percentile_55'] = df[text_col].apply(get_positive_words_55)\n",
    "    df['percentile_50'] = df[text_col].apply(get_positive_words_50)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Basic Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tokenization and Lemmatization\n",
    "\n",
    "First, we need to remove the stop words, punctuation characters and all other special characters from the text.\n",
    "Then, we need to lemmatize the word to it's root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# English Stop Words list\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "corpus = set(nltk.corpus.words.words())\n",
    "\n",
    "# Create a Tag Dictionary, Default tag is Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "# Get a Lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 54s, sys: 1.55 s, total: 6min 55s\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Caution: will take time to lemmatize whole Data\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since, we have fetched document type from name and lower case text from text, we could safely remove these two columns\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "test_df.to_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_df = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Primilinary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Positive and Negative classes are size 71% and 29% respectively. Hence, no severe class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.713089\n",
       "1    0.286911\n",
       "Name: is_fitara, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# confirm class distribution\n",
    "# is_fitara - yes: ~29%; no: ~71%\n",
    "train_df['is_fitara'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 25 words \n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_no_df.head(25))\n",
    "#plt.plot()\n",
    "#plt.figure(figsize=(20,12))\n",
    "#sns.barplot(x = 'word', y = 'frequency', data = exclusive_yes_df.head(25))\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation\n",
    "If ablation is true, use only certain percentage of data for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation:\n",
    "    train_df_no = train_df[train_df.is_fitara == 0]\n",
    "    train_df_yes = train_df[train_df.is_fitara == 1]\n",
    "\n",
    "    # Get 15% of total Records for Ablation\n",
    "    train_df_no_ablation = train_df_no.loc[0:int(len(train_df_no) * ablation_ratio)]\n",
    "    train_df_yes_ablation = train_df_yes.loc[0:int(len(train_df_yes) * ablation_ratio)]\n",
    "    \n",
    "    # Shuffle rows and reset index\n",
    "    train_df = pd.concat([train_df_yes_ablation, train_df_no_ablation]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Split into Test and training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y = train_df['is_fitara']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Percentile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_no_df, exclusive_yes_df = get_top_words(pd.concat([X_train, y_train], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_yes_df_99 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.99)]\n",
    "exclusive_yes_df_95 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.95)]\n",
    "exclusive_yes_df_90 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.90)]\n",
    "exclusive_yes_df_85 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.85)]\n",
    "exclusive_yes_df_80 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.80)]\n",
    "exclusive_yes_df_75 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.75)]\n",
    "exclusive_yes_df_70 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.70)]\n",
    "exclusive_yes_df_65 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.65)]\n",
    "exclusive_yes_df_60 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.60)]\n",
    "exclusive_yes_df_55 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.55)]\n",
    "exclusive_yes_df_50 = exclusive_yes_df[exclusive_yes_df.frequency >= exclusive_yes_df['frequency'].quantile(0.50)]\n",
    "\n",
    "def init_word_dict(df):\n",
    "    df2 = get_top_words_list(df)\n",
    "    \n",
    "    word_dict = dict(zip([word for word in df2.word], \n",
    "                         [re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE) for word in df2.word]))\n",
    "\n",
    "def findWholeWord(word):\n",
    "    return re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_positive_words(text, df):\n",
    "    return int(any([findWholeWord(word)(text) != None for word in df['word']]))\n",
    "\n",
    "def get_positive_words_99(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_99)\n",
    "\n",
    "def get_positive_words_95(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_95)\n",
    "\n",
    "def get_positive_words_90(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_90)\n",
    "\n",
    "def get_positive_words_85(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_85)\n",
    "\n",
    "def get_positive_words_80(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_80)\n",
    "\n",
    "def get_positive_words_75(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_75)\n",
    "\n",
    "def get_positive_words_70(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_70)\n",
    "\n",
    "def get_positive_words_65(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_65)\n",
    "\n",
    "def get_positive_words_60(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_60)\n",
    "\n",
    "def get_positive_words_55(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_55)\n",
    "\n",
    "def get_positive_words_50(text):\n",
    "    return get_positive_words(text, exclusive_yes_df_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_word_dict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 2min 56s, sys: 13.2 s, total: 1h 3min 10s\n",
      "Wall time: 1h 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = add_percentile_features(X_train)\n",
    "X_test = add_percentile_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_percentile.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_percentile.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_percentile.csv'))\n",
    "y_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_label_percentile.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF\n",
    "Create TF IDF Vector representation for Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tf_idf:\n",
    "    X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "    X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "    X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "    X_train.drop([text_col], axis=1, inplace=True)\n",
    "    X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Strings\n",
    "Tokenize the words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tokenizer:\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train[text_col]))\n",
    "    X_train[text_col] = tokenizer.texts_to_sequences(X_train[text_col])\n",
    "    X_test[text_col] = tokenizer.texts_to_sequences(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sentences \n",
    "if tokenizing the words, needs to handle length as well. All the vectors needs to be same length, so pad the ones that are shorter. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tokenizer:\n",
    "    # Find Length of all rows, take a number 50 percentile number\n",
    "    train_max_len = np.percentile(X_train[text_col].apply(len), pad_percentile)\n",
    "    test_max_len = np.percentile(X_test[text_col].apply(len), pad_percentile)\n",
    "\n",
    "    # Get the maximum length\n",
    "    max_len = int(test_max_len if train_max_len < test_max_len else train_max_len)\n",
    "\n",
    "    # Pad the Sentences\n",
    "    X_train_temp = pd.DataFrame(pad_sequences(X_train[text_col], maxlen=max_len), index = X_train.index)\n",
    "    X_test_temp = pd.DataFrame(pad_sequences(X_test[text_col], maxlen=max_len), index = X_test.index)\n",
    "\n",
    "    # Remove Existing DF\n",
    "    X_train.drop([text_col], axis=1, inplace=True)\n",
    "    X_test.drop([text_col], axis=1, inplace=True)\n",
    "\n",
    "    # Create new DF\n",
    "    X_train = pd.concat([X_train_temp, X_train], axis = 1)\n",
    "    X_test = pd.concat([X_test_temp, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>document_type_doc</th>\n",
       "      <th>document_type_docx</th>\n",
       "      <th>document_type_pdf</th>\n",
       "      <th>percentile_99</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoonotic</th>\n",
       "      <th>zoster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>-0.522811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.522014</td>\n",
       "      <td>-0.622241</td>\n",
       "      <td>0.626449</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>-0.263649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.257909</td>\n",
       "      <td>-0.155861</td>\n",
       "      <td>-0.786160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>-0.539960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.537864</td>\n",
       "      <td>-0.676327</td>\n",
       "      <td>1.040523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>-0.527721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.527052</td>\n",
       "      <td>-0.647067</td>\n",
       "      <td>0.547486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>-0.551621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548204</td>\n",
       "      <td>-0.716669</td>\n",
       "      <td>1.704086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>-0.395189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.394213</td>\n",
       "      <td>-0.348265</td>\n",
       "      <td>-0.432301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>-0.519361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.521804</td>\n",
       "      <td>-0.613818</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>-0.527728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.527735</td>\n",
       "      <td>-0.623571</td>\n",
       "      <td>1.125495</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>-0.531842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.530621</td>\n",
       "      <td>-0.649727</td>\n",
       "      <td>0.837050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.728122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824489</td>\n",
       "      <td>1.110280</td>\n",
       "      <td>-1.243369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 10675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_length  capitals  caps_vs_length  num_words  num_unique_words  \\\n",
       "62      -0.522811       0.0             0.0  -0.522014         -0.622241   \n",
       "236     -0.263649       0.0             0.0  -0.257909         -0.155861   \n",
       "809     -0.539960       0.0             0.0  -0.537864         -0.676327   \n",
       "238     -0.527721       0.0             0.0  -0.527052         -0.647067   \n",
       "215     -0.551621       0.0             0.0  -0.548204         -0.716669   \n",
       "..            ...       ...             ...        ...               ...   \n",
       "169     -0.395189       0.0             0.0  -0.394213         -0.348265   \n",
       "545     -0.519361       0.0             0.0  -0.521804         -0.613818   \n",
       "70      -0.527728       0.0             0.0  -0.527735         -0.623571   \n",
       "79      -0.531842       0.0             0.0  -0.530621         -0.649727   \n",
       "371      0.728122       0.0             0.0   0.824489          1.110280   \n",
       "\n",
       "     words_vs_unique  document_type_doc  document_type_docx  \\\n",
       "62          0.626449                  0                   1   \n",
       "236        -0.786160                  0                   0   \n",
       "809         1.040523                  0                   0   \n",
       "238         0.547486                  0                   0   \n",
       "215         1.704086                  0                   1   \n",
       "..               ...                ...                 ...   \n",
       "169        -0.432301                  0                   1   \n",
       "545         0.764925                  0                   0   \n",
       "70          1.125495                  1                   0   \n",
       "79          0.837050                  0                   0   \n",
       "371        -1.243369                  0                   0   \n",
       "\n",
       "     document_type_pdf  percentile_99  ...  zero  zimbabwe  zinc       zip  \\\n",
       "62                   0              0  ...   0.0       0.0   0.0  0.000000   \n",
       "236                  1              0  ...   0.0       0.0   0.0  0.007455   \n",
       "809                  1              0  ...   0.0       0.0   0.0  0.000000   \n",
       "238                  1              0  ...   0.0       0.0   0.0  0.000000   \n",
       "215                  0              0  ...   0.0       0.0   0.0  0.000000   \n",
       "..                 ...            ...  ...   ...       ...   ...       ...   \n",
       "169                  0              0  ...   0.0       0.0   0.0  0.000000   \n",
       "545                  1              0  ...   0.0       0.0   0.0  0.000000   \n",
       "70                   0              0  ...   0.0       0.0   0.0  0.000000   \n",
       "79                   1              0  ...   0.0       0.0   0.0  0.000000   \n",
       "371                  1              0  ...   0.0       0.0   0.0  0.000000   \n",
       "\n",
       "     zone  zoning  zoo  zoom  zoonotic  zoster  \n",
       "62    0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "236   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "809   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "238   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "215   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "..    ...     ...  ...   ...       ...     ...  \n",
       "169   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "545   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "70    0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "79    0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "371   0.0     0.0  0.0   0.0       0.0     0.0  \n",
       "\n",
       "[764 rows x 10675 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "if use_tokenizer:\n",
    "    cols = X_train.columns\n",
    "elif use_tf_idf:\n",
    "    cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'))\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pad.csv'), index_col='Unnamed: 0')\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pad.csv'), index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_columns = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique', \n",
    "        'doc', 'docx', 'pdf']\n",
    "    #, 'percentile_99', 'percentile_95', 'percentile_90',\n",
    "       #'percentile_85', 'percentile_80', 'percentile_75', 'percentile_70',\n",
    "       #'percentile_65', 'percentile_60', 'percentile_55', 'percentile_50']\n",
    "\n",
    "cols = X_train.columns[len(standard_columns):]\n",
    "\n",
    "print(cols)\n",
    "pca = PCA(n_components = 350)\n",
    "\n",
    "pca.fit(X_train[cols])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[cols]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[cols]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Columns\n",
    "X_train.drop(cols, axis=1, inplace=True)\n",
    "X_test.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_pca_2.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_pca_2.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_pca.csv'))\n",
    "y_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_label_pca.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    X_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_train = pd.read_csv(os.path.join(ROOT_DIR, r'train', r'train_label_pca.csv'), header=None, index_col='Unnamed: 0')\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_pca.csv'), index_col='Unnamed: 0')\n",
    "    y_test = pd.read_csv(os.path.join(ROOT_DIR, r'test', r'test_label_pca.csv'), header=None, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "First Try fitting a Simple Neural Network for benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check cross validation score for different algorithms on training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB :: -0.119952 ( +- 0.048334) \n",
      " Log Loss : 1.027896\n",
      " Accuracy : 76.440000\n",
      "\n",
      "GBC :: -0.165445 ( +- 0.068849) \n",
      " Log Loss : 0.913441\n",
      " Accuracy : 75.390000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "estimators = []\n",
    "\n",
    "#estimators.append(('LR', LogisticRegression()))\n",
    "#estimators.append(('NB', BernoulliNB(alpha=.01)))\n",
    "#estimators.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#estimators.append(('KNN', KNeighborsClassifier()))\n",
    "#estimators.append(('TREE', DecisionTreeClassifier()))\n",
    "#estimators.append(('CART', RandomForestClassifier(n_estimators=100)))\n",
    "#estimators.append(('NB', GaussianNB()))\n",
    "#estimators.append(('SVM', SVC(probability=True)))\n",
    "estimators.append(('XGB', XGBClassifier()))\n",
    "estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss')\n",
    "    #cv_results = model_selection.cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    log_loss = metrics.log_loss(y_test, model.predict_proba(X_test))\n",
    "    accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n Log Loss : %f\\n Accuracy : %f\\n\" % (name, cv_results.mean(), cv_results.std(), log_loss, round(accuracy*100, 2))\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, estimators[1][1].predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n",
    "\n",
    "#print(metrics.accuracy_score(y_test.values, estimators[2][1].predict(X_test)))\n",
    "#print(metrics.log_loss(y_test.values, estimators[4][1].predict_proba(X_test)))\n",
    "#math.exp(-metrics.log_loss(y_test.values, estimators[5][1].predict_proba(X_test)))\n",
    "\n",
    "#y_pred = estimators[2][1].predict(X_test)\n",
    "#metrics.roc_curve(y_test.values, y_pred)\n",
    "#estimators[2][1].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(ensemble, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(metrics.log_loss(y_test, ensemble.predict_proba(X_test)))\n",
    "print(metrics.accuracy_score(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, ensemble.predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000, # Very large number\n",
    "    seed=7,\n",
    "    reg_alpha=5,\n",
    "    eval_metric='auc',\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_set=[(X_train, y_train)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(metrics.log_loss(y_test, clf.predict_proba(X_test)))\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test.values, ensemble.predict(X_test)), \n",
    "             columns = [\"Prediction No\",\"Prediction Yes\"], index=[\"Actual No\", \"Actual Yes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n",
    "\n",
    "# combine labels with text features\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# remove dataframes that are no longer needed from memory \n",
    "del train_labels_df\n",
    "del train_text_df\n",
    "\n",
    "train_df['is_fitara'] = train_df['is_fitara'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Add derived features on Train dataframe\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "train_df[text_col] = train_df[text_col].apply(lemmatize)\n",
    "test_df[text_col] = test_df[text_col].apply(lemmatize)\n",
    "\n",
    "to_drop = ['text']\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df['document_type'], prefix='document_type')], axis=1)\n",
    "train_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['document_type'], prefix='document_type')], axis=1)\n",
    "test_df.drop(['document_type'], axis=1, inplace=True)\n",
    "\n",
    "X_train = train_df.drop(['document_name', 'is_fitara'], axis=1)\n",
    "y_train = train_df['is_fitara']\n",
    "\n",
    "X_test = test_df.drop(['document_name'], axis=1)\n",
    "\n",
    "# Percentile Features\n",
    "exclusive_no_df, exclusive_yes_df = get_top_words(pd.concat([X_train, y_train], axis=1))\n",
    "init_word_dict(X_train)\n",
    "X_train = add_percentile_features(X_train)\n",
    "X_test = add_percentile_features(X_test)\n",
    "\n",
    "# TF - IDF Vectorization\n",
    "X_train_tf, X_test_tf, feature_names = get_tf_idf(X_train[text_col], X_test[text_col])\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tf.toarray(), columns = feature_names, index = X_train.index)\n",
    "X_test_df = pd.DataFrame(X_test_tf.toarray(), columns = feature_names, index = X_test.index)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_df], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_df], axis=1)\n",
    "\n",
    "X_train.drop([text_col], axis=1, inplace=True)\n",
    "X_test.drop([text_col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA Dataframe to Disk\n",
    "\n",
    "X_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_X.csv'))\n",
    "y_train.to_csv(os.path.join(ROOT_DIR, r'train', r'train_label_X.csv'))\n",
    "\n",
    "X_test.to_csv(os.path.join(ROOT_DIR, r'test', r'test_X.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Features\n",
    "cols = ['total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique']\n",
    "\n",
    "train_features = X_train[cols].values\n",
    "test_features = X_test[cols].values\n",
    "\n",
    "scaler.fit(train_features)\n",
    "\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "X_train[cols] = pd.DataFrame(train_features, columns = cols, index = X_train.index)\n",
    "X_test[cols] = pd.DataFrame(test_features, columns = cols, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PCA\n",
    "columns = X_train.columns[len(cols):]\n",
    "\n",
    "pca = PCA(n_components = pca_components)\n",
    "\n",
    "pca.fit(X_train[columns])\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.transform(X_train[columns]))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test[columns]))\n",
    "\n",
    "X_train_pca.index = X_train.index\n",
    "X_test_pca.index = X_test.index\n",
    "\n",
    "print(f\"Variance Explained by Model is {pca.explained_variance_ratio_.cumsum()[-1]}\")\n",
    "\n",
    "# Remove Old Columns\n",
    "X_train.drop(columns, axis=1, inplace=True)\n",
    "X_test.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# Append PCA Columns\n",
    "X_train = pd.concat([X_train, X_train_pca], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_pca], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we use sklearn.metrics.log_loss standalone, i.e. log_loss(y_true,y_pred), \n",
    "it generates a positive score -- the smaller the score, the better the performance.\n",
    "\n",
    "However, if we use 'neg_log_loss' as a scoring scheme as in 'cross_val_score\", \n",
    "the score is negative -- the bigger the score, the better the performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "\n",
    "\n",
    "test_estimators = []\n",
    "\n",
    "#test_estimators.append(('LR', LogisticRegression()))\n",
    "test_estimators.append(('XGB', XGBClassifier()))\n",
    "test_estimators.append(('GBC', GradientBoostingClassifier(n_estimators=100)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in test_estimators:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s :: %f ( +- %f) \\n\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(test_estimators, voting='soft')\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "cv_results = cross_val_score(test_estimators, X_train.values, y_train, cv=kfold, scoring='neg_log_loss', verbose=2, n_jobs=-1) \n",
    "\n",
    "print(\"%s: %f (+/- %f)\" % (\"Ensemble\", cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ensemble.predict_proba(X_test))\n",
    "#sub = pd.read_csv(SUB)\n",
    "\n",
    "# Add prediction on Test DF\n",
    "test_df['pred_fitara'] = round(df.iloc[:, 1], 2)\n",
    "\n",
    "submission = test_df.loc[:, ['document_name', 'pred_fitara']]\n",
    "\"\"\"\n",
    "submission = pd.merge(\n",
    "    sub,\n",
    "    test_df,  \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ").loc[:, ['document_name', 'pred']]\n",
    "\n",
    "submission.columns = ['document_name', 'pred_fitara']\n",
    "\"\"\"\n",
    "\n",
    "submission.to_csv(os.path.join(ROOT_DIR, r'submission-4-ensemble-percentile-.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
